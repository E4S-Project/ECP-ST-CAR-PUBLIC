\noindent

\subsubsection{\stid{4.15} ExaIO - ExaHDF5}
\label{subsubsect:exahdf5}

\paragraph{Overview} 
Hierarchical Data Format version 5 (HDF5) is the most popular high-level I/O library for scientific applications to write and read data files.
The HDF Group released the first version of HDF5 in 1998 and over the past 20 years, it has been used by numerous applications not only in scientific domains but also in finance, space technologies, and many other business and engineering fields. HDF5 is the most used library for performing parallel I/O on existing HPC systems at the DOE supercomputing facilities. NASA gives HDF5 software the highest technology readiness level (TRL 9), which is given to actual systems flight-proven through successful mission operations. In ECP, numerous applications declared that HDF5 is a critical dependency for performing I/O.

The ExaIO project's ExaHDF5 team, which includes researchers and developers from LBNL, ANL, and The HDF Group, has developed various HDF5 features to address efficiency and other challenges posed by data management and parallel I/O on exascale architectures. The ExaIO-ExaHDF5 team is productizing features and techniques that have been previously prototyped, exploring optimization strategies on upcoming architectures, maintaining and optimizing existing HDF5 features tailored for ECP applications. Along with supporting and optimizing the I/O performance of HDF5 applications, new features in this project include transparent data caching in the multi-level storage hierarchy, topology-aware I/O performance optimization, asynchronous I/O, multi-dataset I/O API, querying data and metadata, and scalable sub-file I/O. 

Many of the funded exascale applications and codesign centers require HDF5 for their I/O, and enhancing the HDF5 software to handle the unique challenges of exascale architectures will be instrumental in the success of ECP. For instance, AMReX, the AMR codesign center, is using HDF5 for I/O, and all the ECP applications that are collaborating with AMReX will benefit from improvements to HDF5. The EQSIM project updated its software stack to use HDF5 as a file format for their workflows, achieving significant performance benefits and storage space savings. The ExaIO HDF5 team has worked with numerous ECP AD teams, including E3SM, FLASH, WarpX, ExaSky, etc. and with various DOE supercomputing facilities in achieving superior I/O performance with their HDF5 usage and in integrating enhanced HDF5 features in their codes.
The virtual Object Layer (VOL) and interoperability features with PnetCDF and ADIOS data open up the rich HDF5 data management interface to science data stored in other file formats. The dynamically pluggable Virtual File Driver (VFD) feature allows extending HDF5 file format to new sources and destinations of I/O, including GPU memory and cloud storage. The ExaIO HDF5 project has been releasing these new features in HDF5 for broad deployment on HPC systems. Focusing on the challenges of exascale I/O, technologies will be developed based on the massively parallel storage hierarchies that are being built into pre-exascale systems. The enhanced HDF5 software will achieve efficient parallel I/O on exascale systems in ways that will impact a large number of DOE science as well as industrial applications.

\paragraph{Key Challenges}
There are challenges in developing I/O strategies for efficiently using a hierarchy of storage devices and topology of compute nodes, developing interoperability features with other file formats, and integrating existing prototyped features into production releases. 

\subparagraph{Efficient Use of Hierarchical Storage and Topology} Data generation (e.g., by simulations) and consumption (such as for analysis) in exascale applications may span various storage and memory tiers, including near-memory NVRAM, SSD-based burst buffers, fast disk, campaign storage, and archival storage. Effective support for caching and prefetching data based on the needs of the application is critical for scalable performance. Also, support for higher bandwidth transfers and lower message latency interconnects in supercomputers is becoming more complex, in terms of both topologies as well as routing policies. I/O libraries need to fully account for this topology in order to maximize I/O performance, and current I/O mechanisms fail to exploit the system topology efficiently.

%\textit{Interoperability with other file formats.} HDF5 offers a rich data model and powerful features for operating on data, and using these capabilities to access data stored in other data formats would be a valuable productivity boost to application developers and workflows. The team is developing  interoperability features that will enable ECP applications to use HDF5 function calls to read data directly from other file formats. Development includes functionality to read “classic” netCDF (including PnetCDF) and ADIOS/BP files, as these formats are in active use in DOE application communities focused on exascale deliverables. 

\subparagraph{Asynchronous I/O} Asynchronous I/O allows an application to overlap I/O with other operations. When an application properly combines asynchronous I/O with nonblocking communication to overlap those operations with its calculation, it can fully utilize an entire HPC system, leaving few or no system components idle. Thus, adding asynchronous I/O to an application's existing ability to perform nonblocking communication is necessary to maximize the utilization of valuable exascale computing resources.


\paragraph{Solution Strategy}
To mitigate these challenges, the team proposes the following actions for each effort.
%%%%%%% MOVED THIS SECTION TO AFTER DESCRIBING ASYNC I/O
%\textit{Utilizing complex compute and storage hardware. } 
%To take advantage of multiple levels of faster storage layers between memory and medium- to long-term storage, we developed Data Elevator. The Data Elevator library intercepts HDF5 file access calls and redirects them to intermediate faster caching storage layers, which future application reads or writes will then access. Data Elevator was extensively tested on burst buffers that were shared by all compute nodes. We are currently testing it with node-local burst buffer layer using UnifyFS. 

%Taking the usage of multiple levels of memory and storage to the next level, we have designed a new virtual object layer (VOL) connector, called Cache VOL. With the usage of VOL infrastructure in HDF5, Data Elevator as well as Cache VOL intercept data read and write calls and move the data transparently between source and destination storage levels. As a result, applications can take advantage of these VOL connectors without modifying their source code and avoid placing a burden on users to move the data explicitly to and from intermediate caching storage layers.

%In our prior work, improved communication times were achieved for a broad spectrum of data movement patterns such as those seen in multi-physics codes, parallel I/O aggregation, and in situ analysis, and have also improved the time to access parallel file systems. In addition to our work on Data Elevator, the team is also developing these topology-aware optimization strategies as a Virtual File Driver (VFD), which can be plugged into HDF5. 

\begin{wrapfigure}[22]{r}{0.51\textwidth}
  \begin{center}
    %\includegraphics[width=0.48\textwidth]{projects/2.3.4-DataViz/2.3.4.15-HDF5-UnifyCR/VOL-Overview.pdf}
    \includegraphics[width=0.40\textwidth]{projects/2.3.4-DataViz/2.3.4.15-HDF5-UnifyCR/async_io_overview.pdf}
  \end{center}
  \caption{An overview of asynchronous I/O as an HDF5 VOL connector.}
  \label{fig:asyncio-overview}
\end{wrapfigure}

%\textit{Interoperability with other file formats.} To open the HDF5 API for interfacing with various file formats and to provide the capability of intercepting HDF5 API calls, a Virtual Object Layer (VOL) feature is being developed. The VOL adds a new abstraction layer internally within the HDF5 library and is implemented just below the public API. The VOL intercepts all HDF5 API calls that access objects in a file and forwards those calls to an “object connector”, which can be pre-linked, or loaded dynamically. A VOL connector can store HDF5 data model objects in a variety of ways. Figure \ref{fig:vol-overview} shows a high-level view of the VOL, where intercepted HDF5 API calls can interface with other file formats and object storage. We will create VOL connectors to access the netCDF and ADIOS/BP file formats, so that applications can use the HDF5 API to operate on data stored in these formats. The interoperability functions in the VOL support the pre-defined datatypes (integers, floating-point values, strings, etc.) in these formats, and will also support compound datatypes, i.e., a user-defined combination of pre-defined datatypes, in netCDF and will use compound datatypes to support ADIOS’ complex datatypes, which represent complex numbers.

\subparagraph{Asynchronous I/O Virtual Object Layer (VOL) Connector} Overlapping the latency incurred in I/O operations with computation or communication operations can hide the I/O latency and improve application performance significantly. Implementation of asynchronous I/O operations can be achieved in different ways. Since the native asynchronous interface offered by most existing operating systems and low-level I/O frameworks (POSIX AIO and MPI-IO) does not include all file operations, I/O operations are executed using a background thread. With the recent increase in the number of available CPU threads per processor, it is now possible to use a thread to execute asynchronous operations from the core that the application is running on without a significant impact on the application's performance. As shown in Figure \ref{fig:asyncio-overview}, when an application enables asynchronous I/O, a background thread is started. Each I/O operation is intercepted, and an asynchronous task is created, storing all the relevant information before inserting it into the asynchronous task queue. The background thread monitors the running state of the application, and only starts executing the accumulated tasks when it detects the application is idle or performing non-I/O operations. When all I/O operations have completed and the application issues the close file call, the asynchronous I/O related resources, as well as the background thread itself, are freed.

\subparagraph{Utilizing Complex Compute and Storage Hardware} 
To take advantage of multiple levels of faster storage layers between memory and medium- to long-term storage, the ExaIO HDF5 team has upgraded a previously developed Data Elevator VOL connector with a new VOL connector called Cache VOL. The Cache VOL intercepts HDF5 file access calls and redirects them to intermediate faster caching storage layers, which future application reads or writes will then access. Data Elevator was extensively tested on burst buffers that were shared by all compute nodes. The team is currently testing it in combination with the asynchronous I/O VOL connector and using a node-local burst buffer layer using UnifyFS. 

%Taking the usage of multiple levels of memory and storage to the next level, we have designed a new virtual object layer (VOL) connector, called Cache VOL. With the usage of VOL infrastructure in HDF5, Data Elevator as well as Cache VOL intercept data read and write calls and move the data transparently between source and destination storage levels. As a result, applications can take advantage of these VOL connectors without modifying their source code and avoid placing a burden on users to move the data explicitly to and from intermediate caching storage layers.

\paragraph{Recent Progress}
Recent progress includes developments in VOL framework integration into HDF5, the asynchronous I/O VOL connector, multilevel storage capabilities, and ECP application I/O.

\subparagraph{Integration of the VOL framework into the HDF5 develop branch} The initial implementation of the VOL feature introduced in the HDF5 1.12.0 release didn't support stack-able VOL connectors. VOL architecture in HDF5 was updated to allow stacking multiple VOL connectors and is now available in the main HDF5 development branch \url{https://github.com/HDFGroup/hdf5}. A pass-through VOL connector \url{https://github.com/hpc-io/vol-external-passthrough} also has been developed to test the stack-ability of multiple VOL connectors. 

\subparagraph{Asynchronous I/O VOL Connector}  The ExaIO HDF5 team has shown the benefits of using the asynchronous I/O in HDF5 in a TPDS paper (H. Tang, Q. Koziol, S. Byna and J. Ravi, ``Transparent Asynchronous Parallel I/O using Background Threads'' in IEEE Transactions on Parallel and Distributed Systems, vol. , no. 01, pp. 1-1, 5555.
doi: 10.1109/TPDS.2021.3090322). The asynchronous I/O VOL is available for use: \url{https://github.com/hpc-io/vol-async}.

\subparagraph{Developed Methods to Use Multilevel Storage} The project team has developed a prototype implementation of using memory and storage layers for caching data. The team previously demonstrated that using the burst buffers on Cori, where the Data Elevator achieves 1.2--3X performance improvement over a highly tuned HDF5 code in reading data. The ExaIO HDF5 team also developed a Cache VOL connector for taking advantage of RAM, node-local cache or any other layer between application buffer and longer-term storage. The current version of the Cache VOL connector is available for testing: \url{https://github.com/hpc-io/vol-cache}.  

\subparagraph{Supporting ECP Application I/O}
The ExaIO HDF5 team has been working with various applications in the ECP portfolio. Applications in the AMReX codesign center have seen some performance issues, mainly because of less optimal configurations, such as using too few file system servers (e.g. Lustre Object Storage Targets or OSTs), producing a large number of metadata requests, using MPI collective buffering that was observing poor performance on NERSC's Cori. By simply changing these configurations, HDF5 achieved higher performance in writing files. The team also tuned HDF5's I/O performance by more than 10X by setting the alignment parameter that matches the block size of the GPFS file system on Summit at the OLCF. The team extended supporting ECP applications, such as E3SM, WarpX/OpenPMD, and FLASH codes by identifying I/O performance bottlenecks and mitigating them. The team has shown improving I/O performance for I/O benchmarks representative of these applications by up to 8X and is in the process of integrating them into the application codes.

%\emph{Asynchronous I/O}
%The ExaIO-HDF5 team evaluated the asynchronous I/O VOL connector on Cori at NERSC and on Summit at OLCF with several I/O kernels and ECP application I/O benchmarks. Experimental results show that asynchronous I/O can effectively mask the I/O cost when the application is idle or performing non-I/O operations. 

The ExaIO HDF5 team has been building and testing the new HDF5 features on various EAS systems including Perlmutter, Spock, Crusher, and Presque. The asynchronous I/O and cache VOL features, and the h5bench were successful in building. All small-scale tests passed with the develop branch of HDF5. 

\paragraph{Next Steps}

The ExaIO HDF5 team is developing subfiling for reducing locking and contention on parallel file systems,  fine-tuning asynchronous I/O and the Cache VOL connectors for caching and prefetching, and supporting ECP AD and ST teams and facilities in improving the overall performance of HDF5. The team is also working on tuning parallel compression and developing multi-dataset I/O API in HDF5. The team will continue testing and measuring performance benefits on EAS systems. 
