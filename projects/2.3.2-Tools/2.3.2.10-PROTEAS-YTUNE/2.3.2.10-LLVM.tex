\subsubsection{\stid{2.10} PROTEAS-TUNE: LLVM} 

\paragraph{Overview}

LLVM, winner of the 2012 ACM Software System Award, has become an integral part of the software-development ecosystem for optimizing compilers, dynamic-language execution engines, source-code analysis and transformation tools, debuggers and linkers, and a whole host of programming-language and toolchain-related components. Now heavily used in both academia and industry, where it allows for rapid development of production-quality tools, LLVM is increasingly used in work targeted at high-performance computing. LLVM components are integral parts of the programming environments on our upcoming Exascale systems, and smaller-scale systems as well, being not only popular open-source dependencies, but are critical parts of the commercial toolchains provided by essentially all relevant vendors.

\paragraph{Key Challenges}

LLVM is well suited to the compilation of code from C++ and other languages on CPU hardware, and for some models, GPU hardware, but lacks the kind of high-level optimizations necessary to enable performance-portable programming across future architectures.
\begin{itemize}
\item LLVM lacks the ability to understand and optimize parallelism constructs within parallel programs.
\item LLVM lacks the ability to perform high-level loop transformations to take advantage of complex memory hierarchies and parallel-execution capabilities.
\end{itemize}

Without these abilities, code compiled well for LLVM must be presented to the compiler in a form already tuned for a specific architecture, including expressions of parallelism suited for the particular characteristics of the target machine. It is, however, unfeasible to tune our entire workload of applications in this way for multiple target architectures. Autotuning helps this problem by allowing dynamic analysis to supplement static cost modeling, which is always fundamentally limited, but without the ability to perform complex transformations, both the parallel and serial execution speed of the resulting programs will be suboptimal.

There are two remaining challenges that we are addressing: The first is that deploying autotuning relying on source-to-source transformations is difficult because maintaining these separate source kernel versions is practically difficult. The second is that, as a general matter, performance improvements can be obtained by specializing code and runtime as opposed to limiting ourselves to ahead-of-time code generation.

\paragraph{Solution Strategy}
We are developing two significant enhancements to LLVM's core infrastructure, and many other LLVM components. These enhancements are grouped into two categories:
\begin{itemize}
\item Enhancements to LLVM's inter-procedural analysis, and an improved representation of parallelism constructs, to allow LLVM to propagate information across boundaries otherwise imposed by parallelism constructs, and to allow LLVM to transform the parallelism constructs themselves.
\item Enhancements to LLVM's loop-optimization infrastructure to allow the direction of a sequence of loop transformations to loop nests, exposing these features to users through Clang pragmas (in addition to being available at an API level to tools such as autotuners), enabling those transformations to execute as specified, and otherwise enhancing the loop-optimization infrastructure.
\end{itemize}

As part of this project, we're investigating both fundamental intermediate
representation (IR) level enhancements (as part of the Kitsune development), as
well as runtime call aware optimizations that deal with the classical lowering
of parallelism into runtime calls. The latter mechanism is being implemented
upstream as an OpenMP optimization pass, while the Kitsune work is, at present,
more exploratory.

To address autotuning and the need for code specialization, we are developing a just-in-time compilation technology with integrates naturally with the C++ language as well as embedding of (domain specific) languages into C/C++ programs.

\paragraph{Recent Progress}

For parallelism, we have implemented several new features in upstream LLVM
including an OpenMP-aware optimization pass that performs various optimizations
specific to OpenMP code on the host and device (=GPU)~\cite{OpenMPOpt2020}. It
is run by default with medium and high optimizations enabled (``-O2'' and
``-O3''). In addition to transformations it will provide user feedback in form
of optimization remarks (``-Rpass=openmp-opt'')

Extension to the Attributor inter-procedural optimization framework that
transparently applies transformations across the  boundary between sequential
and parallel code. This upstream work will reduce the overhead parallelism
introduces due to missed classical optimizations~\cite{giorgis2020}.

We prototyped heterogeneous LLVM-IR modules which allow host and device code to
coexist in the same LLVM-IR file and therefore be optimized with a holistic
view. Our approach was already discussed with the community and needs to be
further refined and tested.

For loop optimizations, we have implemented several new features in LLVM and
Clang, including the OpenMP 5.1 ``tile'' directive and clang pragma syntax for
exploration of future transformations not yet available through the OpenMP
standard. Most of these enhancements are in papers
(~\cite{kruse2018user,kruse2018loop} and in several forums directly to the LLVM
community.

In a more forward looking approach we prototyped a loop-hierarchical IR for
LLVM which we also present and discuss in various LLVM community forums.

To facilitate autotuning (ref. Section \ref{sec:PROTEAS_TUNE_AUTOTUNING}), we
implemented a loop nest information extraction tool for
LLVM-IR~\cite{kruse2020search}.

We have developed a prototype C++ compiler, based on Clang, supporting an
extension that enables programmers to embed (domain specific) languages inside
their ``C-like'' programs~\cite{finkel2020dsl}. That is, we allow a new type of
Clang plugin to bridge the gap between classical code, e.g., C or C++, and code
written in a different language, e.g., a quantum or tensor domain specific
language (DSL). The latter two examples have been successfully prototyped as
well.

All our efforts have also been featured in many talks, tutorials, and so on at
LLVM developers' meetings over the last couple of years.

\paragraph{Next Steps}
We will continue to prototype implementations, discuss them with the LLVM
community, and then refine them for integration in upstream LLVM.

For the C++ JIT technology, we will also continue to pursue standardization at
the C++ standards committee.

In addition, we are implementing autotuning technology based on the loop
transformation improvements, and other improvements developed by this project.
This will enable an easy-to-use autotuning capability for applications on
Exascale systems.

Parallelism specific optimizations will further be improved through Attributor
enhancements upstream and more capabilities for the OpenMP-aware aware
optimization pass. Generalization of the latter to other parallel models is
planned as well.

To enable optimizations across the host-device boundary we are continuing to
work on heterogeneous LLVM-IR modules in order to integrate them into upstream
LLVM.


%\end{document}
