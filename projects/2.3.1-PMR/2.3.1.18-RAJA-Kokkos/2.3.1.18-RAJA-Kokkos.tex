%  The RAJA text is out-of-date.  Need to comment it out.
%
%\subsubsection{\stid{1.18} ISC4MCM (RAJA)} 
%
%\paragraph{Overview.} 
%The Integrated Software Components for Managing Computation and Memory 
%Interplay at Exascale (ISC4MCM) project is providing software libraries that 
%enable application and library developers to meet advanced architecture 
%portability challenges. The project goals are to enable writing performance 
%portable computational kernels and coordinate complex heterogeneous memory 
%resources among components in a large integrated application. These 
%libraries enhance developer productivity by insulating them from much of the 
%complexity associated with parallel programming model usage and 
%system-specific memory concerns.
%
%The software products provided by this project are three complementary and 
%interoperable libraries:
%\begin{enumerate}
%\item {\bf RAJA:} Software abstractions that enable C++ developers to write
%  performance portable (i.e., single-source) numerical kernels (loops). 
%\item {\bf CHAI:} C++ ``managed array'' abstractions that enable transparent
%  and automatic copying of application data to execution memory spaces at run
%    time as needed based on RAJA execution contexts.
%\item {\bf Umpire:} A portable memory resource management library that provides
%  a unified high-level API for resource discovery, memory provisioning,
%    allocation, access, operations, and introspection.
%\end{enumerate}
%
%Capabilities delivered by these software efforts are needed to manage the
%diversity and uncertainty associated with current and future HPC architecture
%design and software support. Moving forward, ECP applications and libraries 
%need to achieve performance portability: without becoming bound to particular
%(potentially-limiting) hardware or software technologies, by insulating 
%numerical algorithms from platform-specific data and execution concerns, and 
%without major disruption as new machine, programming models, and vendor
%software become available.
%
%These libraries in development in this project are currently used in production
%ASC applications at Lawrence Livermore National Laboratory (LLNL). They are
%also being used or being explored/adopted by several ECP application and
%library projects, including: LLNL ATDM application, GEOS (Subsurface), SW4
%(EQSIM), MFEM (CEED co-design), and SUNDIALS.
%
%The software projects are highly-leveraged with other efforts. Team members
%include: ASC and ATDM application developers, ASD tool developers, university
%collaborators, and vendors. This ECP ST project supports outreach to the ECP
%community and collaboration with ECP efforts.
%
%\paragraph{Key Challenges.}
%
%The main technical challenge for this project is enabling production
%applications to achieve performance portability in an environment of rapidly
%changing, disruptive HPC hardware architecture design. Typical large
%applications contain $O(10^5) - O(10^6)$ lines of code and $O(10K)$ loop
%kernels. The codes must run efficiently on platforms ranging from laptops to
%commodity clusters to large HPC platforms. The codes are long-lived and are
%used daily for decades, so they must be portable across machine generations.
%Also, the codes are under continual development, with a steady stream of new
%capabilities added throughout their lifetimes -- continual validation and
%verification is essential, which precludes substantial rewrites from scratch.
%Lastly, the complex interplay of multiple physics packages and dozens of
%libraries makes it so that the data required for the full set of components
%needed for a given simulation may not fit into a single system memory space. To
%advance scientific computing capabilities, applications must navigate these
%constraints while facing substantial hardware architecture disruption along the
%road toward Exascale computing platforms. 
%
%While the software provided by this project has a substantial user base at
%LLNL, achieving broader adoption in the ECP (projects without LLNL involvement,
%in particular) is another challenge. The software efforts are funded almost
%entirely by LLNL programs and the majority of their developers work on LLNL
%application projects. So resource limitations is a key issue.
%
%\paragraph{Solution Strategy.}
%
%The software libraries in this project focus on encapsulation and 
%application-facing APIs to insulate users from the complexity and 
%challenges associated with diverse forms of parallelism and heterogeneous 
%memory systems. This approach allows users to exploit new capabilities 
%with manageable rewriting of their applications.
%
%RAJA provides various C++ abstractions for parallel loop execution. It
%supports: various parallel programming model back-ends, such as OpenMP 
%(CPU multithreading and target offload), CUDA, Intel Threading Building Blocks,
%etc.; loop iteration space and data view constructs to reorder, 
%aggregate, tile, and partition loop iterations; complex loop kernel 
%transformations for optimization, such as reordering loop nests, fusing 
%loops, etc. RAJA also supports portable atomic operations, parallel scans, 
%and CPU and GPU shared memory. After loops have been converted to RAJA, 
%developers can explore implementation alternatives via RAJA features without 
%altering loop kernels at the application level.
%
%CHAI provides C++ ``managed array'' abstractions that automatically copy 
%data to execution memory spaces as needed at run time based on RAJA execution 
%contexts. Access to array data in loop kernels looks the same as when using
%traditional C-style arrays.
%
%Umpire provides a portable API for managing complex memory resources by 
%providing uniform access to other libraries and utilities that provide
%system-specific capabilities. Umpire decouples resource allocation from 
%specific memory spaces, allocators, and operations. The memory introspection 
%functionality of Umpire enables applications and libraries to make memory 
%usage decisions based on allocation properties (size, location, sharing 
%between packages, etc.)
%
%All three software libraries are open source and available on
%GitHub~\cite{RAJA-github, CHAI-github, Umpire-github}. There they provide
%regular software and documentation releases. Each project has dedicated email
%lists, issue tracking, test suites, and automated testing.
%
%\paragraph{Recent Progress}
%
%In FY18, CHAI and Umpire have been released as open source software projects
%and they are now developed on GitHub Recent development has focused on 
%user documentation and cleaner integration of these two libraries to give 
%applications more flexible and easy access to their capabilities.
%
%Many new features have been added to RAJA in FY18 to enable flexible
%loop transformations for complex loop kernels via execution policies.
%LLNL applications are assessing this new functionality now in a 
%"pre-release" version; it will be generally available before the end of FY18.
%
%The RAJA Performance Suite~\cite{RAJAPerf-github} was released and made 
%available on Github in January 2018. The Suite is used to assess and track 
%performance of RAJA across programming models and diverse loop 
%kernels. It is also being used for compiler acceptance testing in the CORAL 
%procurement and was prepared for use as a benchmark for the CORAL-2 procurement.
%
%In 2018, the RAJA project expanded its visibility beyond DOE NNSA Labs. 
%Recent presentations include a RAJA tutorial at the 2018 ECP Annual Meeting 
%and an application use case study the 2018 NVIDIA GPU Tech Conference (GTC). 
%Future tutorials are planned at 2018 ATPESC and GTC 2019. Also, a RAJA paper 
%and $1/2$-day tutorial proposal were submitted to SC18.
%
%\paragraph{Next Steps}
%
%Our next efforts include:
%\begin{enumerate}
%\item {\bf Fill RAJA Gaps:} Not all features are available for all programming
%  model back-ends; as models mature, such as OpenMP4.5, these gaps will be
%    filled.
%\item {\bf Expand RAJA User Guide and Tutorial:} Build example codes and user
%  documentation for latest RAJA features and prepare for future tutorials
%    (ATPESC 2018 and SC18).
%\item {\bf Expand RAJA Performance Suite:} Include kernels that exercise more
%  application use cases and RAJA features.
%\item {\bf Focus RAJA Vendor Interaction:} Work with CORAL vendors to address
%  issues as applications port to the Sierra platform at LLNL; establish early
%    interactions with CORAL-2 vendors to ensure RAJA will be supported well on
%    CORAL-2 systems.
%\item {\bf Expand Umpire Capabilities:} Explore potential collaboration with
%  relevant ECP efforts, such as SICM project.
%\end{enumerate}
\subsubsection{\stid{1.18} RAJA/Kokkos} 

\paragraph{Introduction}
The RAJA/Kokkos sub-project is a new combined effort intended to focus on collaborative development of backend capabilities for the Aurora and Frontier platforms.  The formation of this project is significant in that it brings two independent teams, RAJA (primarily from LLNL) and Kokkos (primarily from Sandia), to work on a common goal. This project also enhances interactions with staff from other labs, in particular Argonne and Oak Ridge, to help integrate RAJA and Kokkos into the software stack and applications at the respective leadership computing facilities.  The remainder of this section is focused on the Kokkos-specific activities. A description of RAJA is provided in the NNSA/LLNL section 2.3.6.02.


\paragraph{Overview} 

The Kokkos C++ Performance Portability Ecosystem is a production-level solution for writing modern C++ applications in an hardware-agnostic way.
Started by Sandia National Laboratories, it is now supported by developers at the Argonne, Berkeley, Oak Ridge, Los Alamos and Sandia National Laboratories as well as the Swiss National Supercomputing (Centre).
It is now used by more than a hundred HPC projects, and Kokkos-based codes are running regularly at-scale on half of the top ten supercomputers in the world. 
The EcoSystem consists of multiple libraries addressing the primary concerns for developing and maintaining applications in a portable way.
The three main components are the Kokkos Core Programming Model, the Kokkos Kernels Math Libraries and the Kokkos Tools.
Additionally, the Kokkos team is participating in the ISO C++ standard development process, to get successful concepts from the Kokkos EcoSystem incorporated into the standard. 
Its development is largely funded as part of the Exascale Computing Project, with a mix of NNSA ATDM and Office of Science sources. 

 
\paragraph{Key Challenges}

One of the biggest challenges for the ExaScale supercomputing era is the proliferation of different computer architectures, and their associated mechanisms to program them.
Vendors have an incentive to develop their own models in order to have maximum freedom of exposing special hardware capabilities, and potentially achieve "vendor-lock-in".
This poses the problem for applications that they may need to write different variants of their code for different machines - an effort which can be simply not feasible for many of the larger application and library projects.

The Kokkos project aims at solving this issue by providing a programming solution which provides a common interface build upon the vendor specific software stacks.
There are a number of technical challenges associated with that. 
First an abstraction must be designed which is restricted enough to allow mapping to a wide range of architectures while allowing exploitation of all the hardware capabilities provided by new architectures. 
Secondly, the development of support for a new architecture may take significant resources. In order to provide a timely solution for applications in line with the availability of the machine, CoDesign collaborations with the vendors are critical.
At the same time software robustness, quality and interface stability is of utmost importance. 
In contrast to libraries such as the BLAS, programming models permeate the entire code base of an application, and are not isolated to simple call sites. 
API changes thus would require a lot of work inside of the users code base. 
A fourth challenge is that in order to debug and optimize the code base tools are required to gain insights into the application. 

Besides the technical challenges, 
a comprehensive support and training infrastructure is absolutely critical for a new programming model to be successful.
Prospective users must learn how to use the programming model, current users must be able to bring up issues with the development team and access detailed documentation, and the development team of the model must be able to continue technical efforts without being completely saturated with support tasks. 
The latter point became a significant concern for the Kokkos team with the expected growth of the user base through ECP.  
Already before the launch of ECP, there were multiple application or library teams starting to use Kokkos for each developer on the core team -- a level not sustainable into the future without a more scalable support infrastructure. 
This issue was compounded by the fact that Kokkos development was funded through NNSA projects, making it hard to justify extensive support for open science applications. 

\paragraph{Solution Strategy}

To address the challenges the Kokkos team is developing a set of libraries and tools which allow application developers to implement, optimize and maintain performance portable codes. 
At its heart the EcoSystem provides the Kokkos Core Programming Model.
Kokkos Core is a programming model for parallel algorithms that use many-core chips and share memory among those cores.
The programming model includes abstractions for frequently used parallel execution patterns, policies that provide details for how those patterns are executed, and execution spaces that denote on which execution agents the parallel computation is performed. 
Kokkos Core also provides fundamental data structures with policies that provide details for how those data structures are laid out in memory, memory spaces that denote in which memory the data reside, and data access traits conveying special data access semantics.
The model works by requiring that application development teams implement their algorithms in terms of Kokkos’ patterns, policies, and spaces. 
Kokkos Core can then map these algorithms onto the target architecture according to architecture-specific rules necessary to achieve best performance.

Kokkos Kernels is a software library of linear algebra and graph algorithms used across many HPC applications to achieve best (not just good) performance on every architecture. The baseline version of this library is written using the Kokkos Core programming model for portability and good performance. The library has architecture-specific optimizations or uses vendor-specific versions of these mathematical algorithms where needed. This reduces the amount of architecture-specific software that an application team potentially needs to develop, thus further reducing their modification cost to achieve “best in class” performance. 

Kokkos Tools is an innovative “plug in” software interface and a growing set of performance measurement and debugging tools that plug into that interface for application development teams to analyze the execution and memory performance of their software. Teams use this performance profiling and debugging information to determine how well they have designed and implemented their algorithms and to identify portions of their software that should be improved. Kokkos Tools interfaces  leverage the Kokkos Core programming model interface to improve an application developer’s experience dramatically, by forwarding application specific information and their context within the Kokkos Core programming model to the tools.

Kokkos Support addresses the challenges of establishing, growing and maintaining the user community.
First and foremost, it provides explicit means for supporting all DOE ECP applications. 
A main component of that is funding for local Kokkos experts at the Sandia, Oak Ridge, Argonne, Berkeley and Los Alamos laboratories which can serve as direct contacts for local applications and the users of the leadership computing facilities. 
Secondly, the project develops and maintains a reusable support infrastructure, which makes supporting more users scalable and cost effective. 

The support infrastructure consists of GitHub wiki pages for the programming guide and API reference, GitHub issues to track feature requests and bug reports, a Slack channel for direct user-to-user and user-to-developer communication, and tutorial presentations and cloud-based Kokkos hands-on exercises. 

The Kokkos Team is also actively engaging the ISO C++ Committee, where it provides about a third of the members interested in HPC.
This strong engagement enables the team to lead or contribute to numerous proposals.
Among those proposals the team leads are abstractions for multi dimensional arrays based on Kokkos View, atomic operations on generic types and linear algebra algorithms based on Kokkos Kernels, which cover not only the classic Fortran BLAS capabilities, but also batched BLAS and mixed precision linear algebra.
The team also has a central role in the primary proposal introducing heterogeneous computing into the C++ standard via the executors concept.

Furthermore, certain areas of common needs between RAJA and Kokkos have emerged. 
To avoid duplicated efforts, and leverage possible synergies the two teams are developing certain capabilities together.
These include for example:
\begin{itemize}
 \item advanced atomic support with memory order and memory scope exposure.
 \item common metaprogramming facilities.
 \item optional integration of Umpire memory pools into Kokkos.
 \item integration of Kokkos Tools callback mechanisms into RAJA.
 \item an extension of the RAJA performance test suite to include Kokkos variants.
\end{itemize}

\paragraph{Recent Progress}

The Kokkos project now consists of an integrated developers team spanning five DOE National Laboratories.
In particular both NNSA and Office of Science funded developers are working based off the same task and code management system, use a shared slack channel, and attend a common weekly team meeting.
This ensures that no duplication of effort happens, and makes Kokkos a true inter laboratory project.

Kokkos is used by many applications in production across the entire spectrum of DOE's super computers.
Support for current production platforms is mature and stable.
Work on supporting the upcoming ExaScale platforms is underway and the primary Kokkos capabilities for AMD GPUs and Intels GPUs are working. 
Initial application tests were successfully conducted with projects such as EXAALT/LAMMPS, ArborX and Cabana.

A training course was developed called "The Kokkos Lectures", which consists of about 15 hours of recorded lessons and over 20 hands-on exercises.
It is available at \url{https://kokkos.link/the-lectures}.
The \url{https://kokkosteam.slack.com} channel has grown dramatically in use, with about 500 users at the end of 2020, of which more than 150 are active in any given week.
The team finished developing a full API documentation as well as adding use case descriptions for common patterns found in applications.

Auto-tuning is now available as an integrated capability into Kokkos with user facing hooks, which allow for the development of custom tuning tools.

At the C++ committee, the MDSpan proposal is now in wording review - meaning that the technical design is approved. 
MDSpan will be able to provide all the core capabilities of Kokkos\:\:View.
This includes compile and runtime extents, customizable layouts, and data access traits.
The extension to heterogeneous memory can be achieved by trivial extensions. 
Furthermore, the atomic\_ref proposal was voted into C++20.
This capability will provide atomic operations on generic allocations as powerful as Kokkos' atomic operations.
In particular it allows atomic operations on types independent of their size, and not just the ones native in the hardware.
A very recent development, is the proposal for linear algebra functions.
It entails functionality covering all of BLAS 1, 2, and 3, but extends it to any scalar types (including mixing of scalar types) and batched operations.
The proposal was approved by the relevant study groups, as well as the library evolution incubator group.
The Kokkos team was also able to gain co-authors from NVIDIA, Intel and AMD - providing significant support from the leading hardware vendors.

Implementations of those proposals are now available on GitHub. 
The new atomic operations implementation is hosted at \url{https://github.com/desul/desul} which serves as the common utility repository for both Kokkos and RAJA.
RAJA integrated the Kokkos Tools callback interface, which allows it to leverage investment into tools made by the Kokkos effort.

\paragraph{Next Steps}

Highest priority for both RAJA and Kokkos is now the further maturing of the backends for Aurora and Frontier, as well as optimization work guided by application teams.
Besides our general support for application teams, we have chosen a few driver projects to focus the optimization efforts.

Addressing latency limitations in current application design is another critical topic.
Many codes are now reaching a point on the production platforms, where kernel launch, memory transfer, and communication latencies are limiting factors.
The Kokkos team is exploring concepts such as predefined kernel graphs as well as global arrays style communication to address these issues.
Initial prototypes are available now, and need to be tested by applications.

Further work on the shared facilities for RAJA and Kokkos is ongoing.

