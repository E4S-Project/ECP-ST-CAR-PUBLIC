\subsection{\stid{5} \ecosystem}\label{subsect:ecosystem}

\textbf{End State:} A production-ready software stack delivered to our facilities, vendor partners, and the open source HPC community.

\subsubsection{Scope and Requirements}
The focus of this effort is on the ``last mile'' delivery of software that is intended to be supported by DOE Facilities and/or vendor offerings. The scope of this effort breaks down into the following key areas:
\begin{itemize}
	\item Hardening and broad ST and facility adoption of Spack for easy build of software on all target platforms
        \item Delivery of formal software releases (Extreme-Scale Scientific Software Stack, or E4S) using muliple packaging technologies - namely Spack Stacks for from-source builds, and Containers.
	\item Oversight of the ST SDKs (Software Development Kits) developed in all five ST L3 areas, with a goal of ensuring the SDKs are deployed as production-quality products at the Facilities, and available to the broader open-source HPC community through coordinated releases
	\item Development of testing infrastructure (e.g., Continuous Integration) in collaboration with HI 2.4.4 (Software Deployment at the Facilities) for use by ECP teams at the Facilities
	\item Development and hardening of new methods for software deployment through the use of container technology
	\item Informal partnerships with the Linux Foundation's OpenHPC project for potential broader deployment of ST technologies in the OpenHPC ecosystem
	\item Co-design of ST solutions with (primarily ATDM) application teams, particularly in the area of programming models, abstractions for performance portability, and optimized use on Exascale systems
	\item System software that is typically provided by the vendors on a platform, or tightly integrated with vendor solutions – including resource managers, low-level runtimes, power management, and support for hierarchical memory at the Operating System (OS) level
	\item Development of Flang through a subcontract with NVIDIA – a first-of-its-kind open source Fortran compiler built on the LLVM toolchain
	\item Research in resilience to understand the impacts of faults on applications, software, and systems
\end{itemize}
A major goal of ST is to ensure that applications can trust that ST products will be available on DOE Exascale systems in a production-quality state, which implies robust testing, documentation, and a clear line of support for each product. This will largely be an integration effort building on both the SDKs project elements defined in each ST L3 area, and tight collaboration and coordination with the Hardware Integration L3 area for Deployment of Software on Facilities (WBS 2.4.4). We will work to develop, prototype, and deliver foundational infrastructure for technologies such as continuous integration and containers in tight collaboration with our DOE facility partners. The ultimate goal is ensuring that the ECP software stack is robustly supported, as well as finding a reach into the broader HPC open-source community – both of which provide the basis for long-term sustainment required by applications, software, Facilities, and vendors who rely upon these products.

Spack is gaining broad adoption in the open source community as an elegant solution toward solving many of the challenges presented by building software with many dependencies. Spack is one of the most visible outward-facing products in this L3 area, and is the basis for the SDK and E4S efforts.

System software in the form of operating systems capabilities and low-level runtimes has historically been built upon a node-centric viewpoint with a global view of the system tied together in a patchwork of add-on tools and resource managers. In order to support higher-level software development, low-level software layers must be provided to address hierarchical and non-uniform memory management, dynamic power management, lightweight threading and process management, low-level distributed data movement (i.e., messaging), I/O forwarding, and resilience and integrity issues. In addition, support for sophisticated resource scheduling and management, including storage, must account for the ability to accomplish increasingly complex workflows (e.g., ensembles, multi-physics, scale bridging, and uncertainly quantification). 

The overarching goal of the resilience and integrity (RI) effort is to keep the application workload running to an acceptably correct solution in a timely and efficient manner on future systems, even in the presence of increasing failures, challenges in I/O scalability for checkpoint/restart, and silent (undetected) errors.

The ATDM projects in this area are largely focused on unique aspects of the NNSA mission and supporting the National Security Application efforts (AD 2.2.5). Those applications are largely ``from scratch'' developments with a high-risk / high-reward goals, and demand deep software co-design, which these ATDM projects are aimed at supporting. Lessons learned from these efforts are important for the broader ecosystem, and the outward-facing results include publications, upstreamed software to commonly used products (e.g. secure JupyterHub, Kokkos), and many of the ATDM project members are also participants in other ECP projects where these lessons are promulgated. 

\subsubsection{Assumptions and Feasibility}
Success in this effort will require a coordinated effort across the entire hardware and software stack – in particular with HI 2.4.4 (Delivery of Software to Facilities) and in some cases, our vendor partners. Recent restructuring of the ECP to formalize this cooperation is a critical first step in enabling our goals, and this area will drive toward ensuring those partnerships can flourish for mutual gain.

Given the project timelines and requirements of production systems at our Facilities, we do not envision a wholly new system software stack as a feasible solution. We do however recognize that in many cases the features of today's HPC operating system environments will very likely need to either be evolved or extended to meet the mission goals. This will require first, proof-of-concept on existing pre-Exascale hardware, and ultimately – adoption of technologies by system vendors where required, and by other application and software teams where user-level (i.e., non-kernel) solutions are developed. 

\subsubsection{Objectives}
This area will focus on all aspects of integration of the ECP software stack, with a focus on putting the infrastructure in place (in partnership with HI and the SDKs) for production-quality software delivery through technologies such as Spack, resource managers, continuous integration, and containers. Likewise, we will aim to influence the deployment of operating system, low-level runtimes, and perhaps containers typically deployed by our vendor partners. Finally, our ATDM projects will focus on delivery and integration of novel software technologies into next-generation applications under development in AD National Security Applications – with a goal of demonstrating and hardening those technologies for possible use in other applications.

Additional goals include providing infrastructure and higher-level tools that address the requirements and extensions for better resource allocation and job scheduling capabilities. These changes will address the necessary aspects of system architectures, including storage resources, and the support for increasingly complex workflows that are projected to occur within the Exascale environment. 

The Flang effort is being developed by NVIDIA's PGI compiler team based on the robust and widely used PGF compiler. Flang was released on GitHub as an open source project in 2017, and is making solid progress toward performance and portability goals. Our objective is to have Flang supported at the DOE Facilities for use by ECP application teams, as well as taken up by vendors (ARM being an early adopter) as a first-class Fortran solution.

The objective of resilience efforts is that applications will run successfully and efficiently to timely completion in the presence of any faults experienced on the system. Application developers will have the necessary programming tools, libraries, and system support for incorporating resilience into their code. This will include access to nonvolatile memory, fault tolerant libraries, and scientific libraries that are resilient to soft errors and support application developers to implement their own resilient algorithms.

\subsubsection{Plan}
Version 0.1 of the Extreme-Scale Scientific Software Stack (E4S) was released in Nov. 2018 comprising of a subset of ST projects which had Spack packages. This release also demonstrated the use of container technologies, with inclusion of Docker, Singularity, Shifter, and CharlieCloud containers for people to use a starting point for integration into applications. In January 2019, version 0.2 of the E4S was released. 2019 and beyond will see a regular cadence of E4S releases, with ever-increasing number of ST products included, broader facility adoption, and potentially inclusion in vendor offerings.

In close coordination with E4S, a number of SDKs are being developed across the other L3 ST areas, building on the years of experience the xSDK (Math Libraries) has built. We expect SDKs to become a prime vehicle for our delivery strategy, while also providing ST products with a standard set of community policies aimed at robust production-ready software delivery. In 2019 and beyond, we plan to define our initial set of SDKs, define community policies, and develop a delivery and deployment mechanism that will get these products into the hands of our application users.

Spack (and it's companion project Spack Stacks) continues to gain penetration across the ECP, and will be the de facto delivery method for ST products building from source. Spack Stacks is a compendium project aimed at making large-scale deployments of a large set of software (e.g. E4S, or facility installs of ST products) easier to manage and fully reproducable.

Flang will build upon some early work in a from-scratch front-end Fortran 2018 parser (called F18) to incorporate that as the production front-end for Flang, replacing some decades-old technology. F18 addresses most of the requirements for Flang to eventually become a standard sub-project in LLVM, and was developed with LLVM standards from scratch. We expect this work to take firm root in 2019, with full adoption in the coming year.

Several new projects in the ecosystem L3 are being planned, and are expected to receive some initial funding in 2019 with a full start in FY20. These include:
\begin{itemize}
 \item Tactics (name TBD) - a pair of assessment teams aimed at closely monitoring exascale efforts outside of ECP, and working with internal ST teams on measuring their impact goals and metrics (KPP-3)
 \item Spack - Spack is currently supported by NNSA (ATDM) funding and a bit from HI. This project will supplement those efforts to accelerate adoption into the rest of ECP
 \item Containers - We plan to coordinate a number of container-related efforts under a new focused project, aimed at the specific needs of HPC and exascale computing not currently addressed in the Container ecosystem
\end{itemize}

\subsubsection{Risks and Mitigations Strategies}
\begin{itemize}
	\item Vendors unwilling to adopt aspects of the Argo environment that require kernel-level support.
	\item Delays in deploying a common CI infrastructure lead to subsequent delays in an integrated software release.
	\item Multiple container technologies in flight will make it hard to come to agreement on a “common” looking solution. Singularity isn't funded by ECP. BEE and Argo are the only ECP funded items, and it is unclear if they are the right final solution.
	\item Resilience work is a tiny fraction of effort. May need to consider a software co-design center around this topic that would marry efforts between apps, ST, and vendors.
	\item ATDM efforts may continue to be inward-facing and not suitable for longer-term broad adoption if/when technologies are successfully borne out in practice.
	\item OpenHPC partnership is ill-defined, and unfunded.
	\item Sustainability of ECP ST capabilities after ECP has ended.
\end{itemize}
