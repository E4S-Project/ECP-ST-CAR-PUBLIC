\subsection{\stid{1}  \pmr}\label{subsect:pmr}

\textbf{End State:} A cross-platform, production-ready programming environment that enables and accelerates the development of mission-critical software at both the node and full-system levels.

\subsubsection{Scope and Requirements}
A programming model provides the abstract design upon which developers express and coordinate the efficient parallel execution of their program. A particular model is implemented as a developer-facing interface and a supporting set of runtime layers. To successfully address the challenges of exascale computing, these software capabilities must address the challenges of programming at both the node- and full-system levels. These two targets must be coupled to support multiple complexities expected with exascale systems (e.g., locality for deep memory hierarchies, affinity for threads of execution, load balancing) and also provide a set of mechanisms for performance portability across the range of potential and final system designs. Additionally, there must be mechanisms for the interoperability and composition of multiple implementations (e.g., one at the system level and one at the node level). This must include abilities such as resource sharing for workloads that include coupled applications, supporting libraries and frameworks, and capabilities such as in situ analysis and visualization. 

Given the ECP’s timeline, the development of new programming languages and their supporting infrastructure is infeasible. We do, however, recognize that the augmentation or extension of the features of existing and widely used languages (e.g., C/C++ and Fortran) could provide solutions for simplifying certain software development activities. 

\subsubsection{Assumptions and Feasibility}
The intent of the PMR L3 is to provide a set of programming abstractions and their supporting implementations that allow programmers to select from options that meet demands for expressiveness, performance, productivity, compatibility, and portability. It is important to note that, while these goals are obviously desirable, they must be balanced with an additional awareness that today’s methods and techniques may require changes in both the application and the overall programming environment and within the supporting software stack.

\subsubsection{Objectives}
PMR provides the software infrastructure necessary to enable and accelerate the development of HPC applications that perform well and are correct and robust, while reducing the cost both for initial development and ongoing porting and maintenance. PMR activities need to reflect the requirements of increasingly complex application scenarios, usage models, and workflows, while at the same time addressing the hardware challenges of increased levels of concurrency, data locality, power, and resilience. The software environment will support programming at multiple levels of abstraction that includes both mainstream as well as alternative approaches if feasible in ECP’s timeframe. 

Both of these approaches must provide a portability path such that a single application code can run well on multiple types of systems, or multiple generations of systems, with minimal changes. The layers of the system and programming environment implementation will therefore aim to hide the differences through compilers, runtime systems, messaging standards, shared-memory standards, and programming abstractions designed to help developers map algorithms onto the underlying hardware and schedule data motion and computation with increased automation.
\subsubsection{Plan}
PMR contains nine L4 projects. To ensure relevance to DOE missions, these efforts leverage and collaborate with existing activities within the broader HPC community. The PMR area supports the research and development needed to produce exascale-ready versions of the Message Passing Interface (MPI);  Partitioned Global-Address Space Libraries (UPC++, GASNet); task-based programming models (Legion, PaRSEC); software for node-level performance portability (Kokkos, RAJA); and libraries for memory, power, and resource management.
Initial efforts focused on identifying the core capabilities needed by the selected ECP applications and components of the software stack, identifying shortcomings of current approaches, establishing performance baselines of existing implementations on available petascale and prototype systems, and the re-implementation of the lower-level capabilities of relevant libraries and frameworks. These efforts provided demonstrations of parallel performance of algorithms on pre-exascale, leadership-class machines--at first on test problems, but eventually in actual applications (in close collaboration with the AD and HI teams). Initial efforts also informed research into exascale-specific algorithms and requirements that will be implemented across the software stack. The supported projects targeted and implemented early versions of their software on CORAL, NERSC and ACES pre-exascale systems--with an ultimate target of production-ready deployment on the exascale systems.
In FY20--23, the focus will be on development and tuning for the specific architectures of the selected exascale platforms, in addition to tuning specific features that are critical to ECP applications.

Throughout the effort, the applications teams and other elements of the software stack evaluate and provide feedback on their functionality, performance, and robustness. Progress towards these goals is documented quarterly and evaluated annually (or more frequently if needed) based on PMR-centric milestones as well as joint milestone activities shared across associated software stack activities by Application Development and Hardware \& Integration focus areas.


\subsubsection{Risks and Mitigation Strategies}
The mainstream activities of ECP in the area of programming models focus on advancing the capabilities of MPI and OpenMP. Pushing them as far as possible into the exascale era is key to supporting an evolutionary path for applications. This is the primary risk mitigation approach for existing application codes. Extensions to MPI and OpenMP standards require research, and part of the efforts will focus on rolling these findings into existing standards, which takes time. To further address risks, PMR is exploring alternative approaches to mitigate the impact of potential limitations of the MPI and OpenMP programming models. 

Another risk is the failure of adoption of the software stack by the vendors, which is mitigated by the specific delivery focus in sub-element SW Ecosystem and Delivery. Past experience has shown that a combination of laboratory-supported open-source software and vendor-optimized solutions built around standard APIs that encourage innovation across multiple platforms is a viable approach and is what we are doing in PMR. We are using close interaction with the vendors early on to encourage adoption of the software stack, including well-tested practices of including support for key software products or APIs into large procurements through NRE or other contractual obligations. A mitigation strategy for this approach involves building a long-lasting open-source community around projects that are supported via laboratory and university funding. 

Creating a coordinated set of software requires strong management to ensure that duplication of effort is minimized. This is recognized by ECP management, and processes are in place to ensure collaboration is effective, shortcuts are avoided unless necessary, and an agile approach to development is instituted to prevent prototypes moving directly to product. 

\subsubsection{Future Trends}
Recently announced exascale system procurements have shown that the
trend in exascale compute-node hardware is toward heterogeneity:
Compute nodes of future systems will have a combination of regular
CPUs and accelerators (typically GPUs). Furthermore, the GPUs will not
be just from NVIDIA as on existing systems: One system will have Intel
GPUs and another will have AMD GPUs. In other words, there will be a
diversity of GPU architectures, each with their own vendor-preferred
way of programming the GPUs. An additional complication
is that although the HPC community has some experience in using NVIDIA
GPUs and the associated CUDA programming model, the community has relatively
little experience in programming Intel or AMD GPUs.  These
issues lead to challenges for application and software teams in
developing exascale software that is both portable and high performance. Below
we outline trends in programming these complex systems that will help
alleviate some of these challenges.

\paragraph{Trends in Internode Programming}
The presence of accelerator hardware on compute nodes has resulted in individual
compute nodes becoming very powerful. As a result, millions of compute
nodes are no longer needed to build an exascale system. This trend
results in a lower burden on the programming system used for internode
communication. It is widely expected that MPI will continue to serve
the purpose of internode communication on exascale systems and is the
least disruptive path for applications, most of which already use
MPI. Nonetheless, improvements are needed in the MPI Standard as well
as in MPI implementations in areas such as hybrid programming
(integration with GPUs and GPU memory, integration with the intranode
programming model), overall resilience and robustness, scalability,
low-latency communication, optimized collective algorithms, optimized support
for exascale interconnects and lower-level communication paradigms
such as OFI and UCX, and scalable process startup and management. PGAS
models, such as UPC++ and OpenSHMEM, are also available to be used by
applications that rely on them and face similar
challenges as MPI on exascale systems. These challenges are being tackled by the MPI and
UPC++/GASNet projects in the PMR area.

\paragraph{Trends in Intranode Programming}
The main challenge for exascale is in achieving performance and portability for
intranode programming, for which a variety of options
exist. Vendor-supported options include CUDA and OpenACC for NVIDIA
GPUs, SYCL/DPC++ for Intel GPUs, and HIP for AMD GPUs. OpenACC
supports accelerator programming via compiler directives. SYCL
provides a C++ abstraction on top of OpenCL, which itself is a
portable, lower-level API for programming heterogeneous
devices. Intel's DPC++ is similar to SYCL with some extensions. HIP
from AMD is similar to CUDA; in fact, AMD provides translation tools
to convert CUDA programs to HIP.

Among portable, standard programming models, OpenMP has supported
accelerators via the \texttt{target} directive starting with OpenMP version
4.0 released in July 2013. Subsequent releases of OpenMP (version 4.5
and 5.0) have further improved support for accelerators. OpenMP is
supported by vendors on all platforms and, in theory, could serve as a
portable intranode programming model for systems with
accelerators. However, in practice, a lot depends on the quality of
the implementation.

Kokkos and RAJA provide another alternative for portable,
heterogenous-node programming via C++ abstractions. They
are designed to work on complex node architectures
with multiple types of execution resources and multilevel memory
hierarchies. Many ECP applications are successfully using Kokkos and
RAJA to write portable parallel code that runs efficiently on GPUs.

We believe these options (and high-quality implementations of them) will
meet the needs of applications in the exascale timeframe. 
