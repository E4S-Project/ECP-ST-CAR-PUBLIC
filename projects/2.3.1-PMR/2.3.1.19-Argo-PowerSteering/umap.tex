\subsubsection*{UMap}

\paragraph{Overview}

UMap is a user level library providing a high performance mmap-like
interface that can be tuned to application needs without requiring OS
customization or impacting system configuration. UMap enables
applications to interact with out-of-core data sets as if in memory,
and to configure parameters such as page buffer size and page size on
a per-application basis.

Leadership supercomputers feature a diversity of storage, from
node-local persistent memory and NVMe SSDs to network-interconnected
flash memory and HDD. Interacting with large persistent data sources
is critical to exascale applications that harness the power of data
analytics and machine learning. The UMap user-level library enables
user-space page management of data located in the memory/storage
hierarchy. By providing a memory map interface, applications can
interact with large data sets as if in memory. As a user level
library, a UMap page fault handler can be easily adapted to access
patterns in applications and to storage characteristics, reducing latency
and improving performance.

\paragraph{Key Challenges}

As ECP applications transition to include ML and data analytics as
integral components of workflows, persistent memories and low latency
storage devices offer new alternatives to hold portions of very large
global data sets within the fabric of the computing system. These new
applications drive new access patterns, i.e. read-dominated analysis
of observational or simulation data rather than write-mostly
checkpoints. The combination of new technologies (byte addressable,
very low latency, asymmetric read/write latency), new insertion points
(node local, Top of Rack or other intermediate storage, global FS,
external distributed storage servers), and new applications (in-situ
analytics, experimental $+$ simulation data analysis, ML batched data
sets) present challenges both to the traditional memory/storage
dichotomy as well as to traditional HPC I/O libraries tailored to
checkpoint transmission.

\paragraph{Solution Strategy}

We prioritize four design choices for UMap based on surveying
realistic use cases. First, we choose to implement UMap as a
user-level library so that it can maintain compatibility with the
fast-moving Linux kernel without the need to track and modify for
frequent kernel updates. Also, we employ the recent \texttt{userfaultfd} 
mechanism, rather than the signal handling + callback function approach
to reduce overhead and performance variance in multi-threaded
applications. Third, we target an adaptive solution that sustains
performance even at high concurrency for data-intensive applications,
which often employ a large number of threads for hiding data access
latency. Our design pays particular consideration on load imbalance
among service threads to improve the utilization of shared resources
even when data accesses to pages are skewed. UMap dynamically balances
workloads among all service threads to eliminate bottleneck on serving
hot pages. Finally, for flexible and portable tuning on different
computing systems, UMap provides both API and environmental controls
to enable configurable page sizes, eviction strategy,
application-specific prefetching, and detailed diagnosis information to the
programmer. The UMap software architecture is shown in Figure
\ref{fig:umaparch}.

\begin{figure}[t]
        \centering
        \includegraphics[scale = 0.5]{projects/2.3.1-PMR/2.3.1.19-Argo-PowerSteering/umap-arch}
        \caption{UMap Handler architecture}
        \label{fig:umaparch}
\end{figure}

\paragraph{Recent Progress}

In recent months, we have released MP-UMap, providing multi-process
support for the UMap facility. MP-UMap enables multiple processes on a
compute node to share the UMap page buffer so that pages accessed from
a mapped file are accessible to MPI ranks on the same node. MP-UMap is
implemented as 
a collection of libraries that allows multiple processes to share a
single file-backed UMap buffer. Just like UMap, it uses user-space
page fault handler based on the userfaultfd Linux feature (starting
with 4.3 linux kernel). The use case is to have a multi-process
application accessing a large file through cached pages,
i.e. out-of-core execution using memory map. Presently, userfaultfd
does not support Write-Protection(WP) of shared memory, which limits
MP-UMap to read-only applications. However, we have identified an
experimental Linux kernel
https://github.com/xzpeter/linux/tree/uffd-wp-shmem-hugetlbfs-v4 where
WP capability is being actively developed. Our feasibility
investigation has shown that this experimental version will help us in
upgrading MP-UMap's capability to support read-write
applications. Thus, going forward we will require capability of
running such experimental Linux versions on EAS platforms to test/use
advanced capabilities like MP-UMap. Ultimately, as has happened
previously, we have helped push adoption of the experimental feature
into the mainstream linux distribution.

The LLNL UMap team demonstrated graph analysis algorithms based on
graph traversal that use the UMap handler.  By integrating with SICMâ€™s
persistent memory C++ allocator called Metall into UMap, the UMap
handler provides memory mapped access to very large graphs dynamically
allocated and stored in binary form in one or more file(s). The
algorithm suite consists of a dynamic graph construction benchmark,
breadth-first search, connected components, and the ExaGraph miniVite
benchmark. Combined use of UMap and Metall require the write-protect
feature of userfaultfd, which is available in the linux 5.10 kernel
and beyond. Currently this version is not available on EAS
machines. Having EAS nodes with newer kernel versions would help with
testing and CI of Metall+UMap enabled ExaGraph applications that have
been demonstrated.

An adaptive buffer management scheme was added to UMap to monitor and
react at runtime to adjust the page buffer size. With this
optimization, the page buffer can grow and shrink to reflect memory
space availability changes.

A paper on UMap was published in IEEE Transactions on Parallel and
Distributed Systems:  Peng, I.B., Gokhale, M., Youssef, K., Iwabuchi,
K. and Pearce, R., 2021. Enabling Scalable and Extensible
Memory-mapped Datastores in Userspace. doi: 10.1109/TPDS.2021.3086302

\paragraph{EAS Experiences}

UMap and MP-UMap have been demonstrated on Frontier architecture EAS
systems. No porting was required as UMap in read-only mode runs on the
standard system software stack. Specifically, UMap was run on a node
of Redwood, an El Capitan COE machine.

  UMap-MP was run Spock as documented in the STPR19-19 Milestone
  report. Functionality of MP-UMap APIs were tested with a Spock Compute
  Node by using test apps and scripts available at \\
\url{https://github.com/LLNL/umap/tree/mpumap/tests/umap-service}.

The full UMap and MP-UMap functionality cannot be run on EAS systems
until the Linux kernel is upgraded to 5.10 for UMap and 5.14 for MP-UMap.

\paragraph{Next Steps}
In the coming year, we plan to continue outreach to application teams
within ECP and in the science/data community. We are in collaboration
with the Umpire team and are integrating UMap with Umpire, a new
capability to be delivered in FY22. UMap with Umpire will give
applications a common API to manage multiple sorts of memories
including persistent memory through UMap and main CPU and GPU memory
resources. The Exagraph collaboration is in progress. Selected
Exagraph algorithms can memory map persistent data such as
binary format graphs and intermediate data structures through UMap
using the SparseStore and Metall.

Recognizing that new forms of memory will be available for Exascale
architectures, we are enhancing the core UMap handler to exploit a
multi-tier memory hierarchy in which pages can migrate between
capacity/latency tiers.

