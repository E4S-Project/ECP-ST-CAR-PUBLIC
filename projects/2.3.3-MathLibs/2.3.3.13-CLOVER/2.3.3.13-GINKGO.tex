\subsubsection{\stid{3.13} CLOVER Sub-project: Ginkgo} \label{subsubsect:peeks}
\paragraph{Overview} 
Ginkgo~\footnote{\url{https://github.com/ginkgo-project/ginkgo}} is a modern
linear algebra library engineered towards performance portability, and
productivity. To achieve these goals, the library design is guided by combining
ecosystem extensibility with heavy, architecture-specific kernel optimization
using the platform-native languages CUDA (NVIDIA GPUs), HIP (AMD GPUs), DPC++
(Intel GPUs) and OpenMP (Intel/AMD/ARM multicore). Ginkgo is part of the
extreme-scale Software development Kit (xSDK), part of the Extreme-Scale
Scientific Software Stack (E4S), and has already been integrated as a backend
into simulation libraries like deal.II, MFEM, and HyTeG.
% SUNDIALS, and XGC.



\paragraph{Key Challenges}
% There is a list of challenges to meet with the development of sparse linear
% algebra functionality for the US flagship supercomputers deployed in ECP:
%\begin{enumerate}
The extreme levels of hardware concurrency available in the
GPU-accelerated nodes must be reflected in fine-grain parallelism in
the numerical building blocks.
To that end, an increasing variety of hardware designs and hardware-native
programming languages requires a library design that enables platform
portability without sacrificing performance.

Applications that build upon the fast solution of many independent
moderate-sized sparse linear systems require batched sparse linear
algebra functionality, and
applications that build upon matrix-free methods require the flexibility
to compose linear solvers out of library-native and customized external
functionality.
Moreover, the arithmetic performance of processors is growing much faster than the
memory bandwidth and interconnect speed, and this requires exploring innovative
strategies to reduce the pressure on all cache/memory levels.
%\end{enumerate}

\begin{figure}[!h]
\centering
\includegraphics[width=.8\columnwidth]{projects/2.3.3-MathLibs/2.3.3.13-CLOVER/ginkgo_portability_crop}
\caption{\label{fig:ginkgoportability}The portability design of the Ginkgo math
  library enables high performance kernel implementations in the vendor-supported
  programming language.}
\end{figure}


\paragraph{Solution Strategy}

The Ginkgo team is addressing these challenges by striving for platform
portability, modularity, extensibility, and hardware-aware algorithm design.

  \subparagraph{Architecture-Portable Software Design}
		Ginkgo~\cite{anzt2020ginkgo} employs a design that decouples the
		algorithm implementations from the hardware-specific kernel
		implementations, thereby acknowledging the importance of platform
		portability and allowing for architecture-specific kernel optimization
		in the vendor language, see Figure~\ref{fig:ginkgoportability}.
  \subparagraph{Modularity, Flexibility, and Extensibility} Ginkgo employs a
		linear operator abstraction for all functionality, which allows for
		flexibility in combining functionality and interfacing external
		operators.
  \subparagraph{Sustainability Efforts} Ginkgo adheres the Better Scientific
		Software (BSSw) design principles~\cite{betterscientificsoftware} that
		ensure production-quality code by featuring unit testing, automated
		configuration and installation, Doxygen code documentation, as well as a
		continuous integration and continuous benchmarking
		framework~\cite{pasc_anzt}. Ginkgo is an open source effort licensed
		under the BSD 3-clause and included in the xSDK and E4S software
		packages.
  \subparagraph{Fine-Grain Parallelism} Ginkgo features linear algebra
		building blocks and advanced algorithms for preconditioning and solving
		linear systems that can efficiently leverage the concurrency of modern
		GPUs, including (incomplete) parallel factorizations, parallel sparse
		triangular solves, parallel matrix operations, and parallel iterative
		methods.
  \subparagraph{Mixed-Precision Methods} Ginkgo features a memory accessor
		that encapsulates on-the-fly compression for decoupling the memory
		precision from the arithmetic precision. This allows accelerating
		memory-bound algorithms that can compensate or tolerate some information
		loss in the memory operations. The memory accessor can also be used to
		increase the result accuracy of memory-bound kernels that benefit from
		using a more complex precision format in the arithmetic operations
		without performance loss.
  \subparagraph{Batched Preconditioned Iterative Solvers} Ginkgo contains high
		performance batched iterative solvers that handle the concurrent
		solution of a set of independent sparse linear systems of moderate size.
		The batched iterative solvers allow for some flexibility in terms of the
		preconditioner and monitor the system-individual convergence without
		performance degradation.

\paragraph{Recent Progress}
%
First, the Ginkgo library realized native support for Intel GPUs (via
		DPC++)~\cite{tsai2021porting}.
Also, MFEM treating Ginkgo as a numerical backend allows accelerating MFEM
		simulations with AMD GPUs, Intel GPUs, and NVIDIA GPUs.
The Ginkgo library has also been expanded with new advanced preconditioners
		such as ISAI and Multigrid, also supporting mixed-precision capabilities~\cite{10.1007/978-3-030-85665-6_34}.
Based on the memory accessor, the Ginkgo team deployed a Compressed
		Basis GMRES (CB-GMRES) solver that outperforms the standard GMRES solver
		by storing the Krylov basis vectors in lower precision, therewith
		accelerating the memory access~\cite{DBLP:journals/corr/abs-2009-12101}.
Finally, the Ginkgo team successfully employed the batched iterative solvers for
		the hydrodynamic problems arising in the PeleLM
		simulations and the gyrokinetic problems arising in the
		XGC simulations~\cite{batched-scala2021}.


\paragraph{Next Steps}
The following efforts have been identified for the next phase of the project.

  \subparagraph{Deployment of Multinode Functionality} Ginkgo already has
		experimental support for multi-node execution via an executor-agnostic
		MPI layer. We will extend the support and make it production-ready.
  \subparagraph{Deployment of GPU-resident Sparse Direct Solvers} In response to
		an urgent need of national power grid simulations, we will develop
		GPU-resident sparse direct solvers.
  \subparagraph{Block-Versions of the Parallel Incomplete Factorization
		Preconditioner} To better reflect the properties of the ECP application
		projects, we will deploy blocked versions of the ParILU and ParILUT
		parallel ILU and parallel threshold ILU preconditioners in the Ginkgo
		software library.
  \subparagraph{Problem-Specific Preconditioners for MFEM} In collaboration
		with the ECP CEED cluster, we will design problem-specific mixed
		precision preconditioners for matrix-free finite element simulations.



\paragraph{Early Access Systems (EAS) Experiences}
Due to its backend model, Ginkgo has successfully been extended to HIP, with
support fully released in version 1.2.0 in July 2020 and DPC++ released in
version 1.4.0 in August 2021. The Ginkgo team has been able to continue the
development of these backends, work on new experimental features (e.g., batched
and distributed), and improve existing functionality performance when needed, in
particular thanks to EAS availability with the Perlmutter, JLSE Arcticus, Spock
and Crusher machines. In the following, we report results obtained on the EAS
Crusher for some of the single node Ginkgo functionality.

On the EAS Crusher system, we evaluate the performance of some of Ginkgo's
iterative solvers and their essential building block, the Sparse Matrix-Vector
product operation (SpMV), on one GCD of a MI250X.~\footnote{For platform
  details, we refer to
  \href{https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html}{ORNL
    Crusher Documentation} and
  \href{https://www.amd.com/system/files/documents/amd-cdna2-white-paper.pdf}{AMD
    CDNA-2 Architecture Whitepaper}.} In these figures, each dot is a matrix
from the SuiteSparse Matrix Collection\footnote{\url{https://sparse.tamu.edu/}}.
In Figure~\ref{fig:gko-crusher-spmv} we show the bandwidth of the Ginkgo SpMV
operation with the CSR and ELL formats. For matrices with a large nonzero count,
both SpMV formats reach up to 87\% of the 1.6~TB/s peak bandwidth for one GCD.
In Figure~\ref{fig:gko-crusher-solvers} we benchmark the bandwidth of the CG,
and GMRES solvers. For each matrix and solver combination, Ginkgo's benchmarking 
setup first runs the SpMV benchmark, selects the fastest matrix format for the matrix
in question and runs the solver benchmark. This solver benchmark consists of 10000
unpreconditioned iterations for each solver type and all results are normalized over a
single iteration. We see that the CG solver reaches a bandwidth of up to 80\% of the
theoretical peak, while on average GMRES can have a slightly lower efficiency and be
more affected by the sparsity pattern of the matrix.


\begin{figure}[!h]
  \centering
  \begin{tabular}{lr}
    \includegraphics[width=.34\columnwidth]{projects/2.3.3-MathLibs/2.3.3.13-CLOVER/ginkgo_eas/bw_csr_spmv.pdf}
    \includegraphics[width=.34\columnwidth]{projects/2.3.3-MathLibs/2.3.3.13-CLOVER/ginkgo_eas/bw_ell_spmv.pdf}
  \end{tabular}
  \caption{Achieved bandwidth (GB/s) of the Ginkgo CSR (left), and ELL (right) SpMV operations on one GCD
    of the Crusher MI250X GPUs.}
  \label{fig:gko-crusher-spmv}
\end{figure}

\begin{figure}[!h]
  \centering
  \begin{tabular}{lr}
    \includegraphics[width=.34\columnwidth]{projects/2.3.3-MathLibs/2.3.3.13-CLOVER/ginkgo_eas/bw_cg_solver.pdf}
    &
    \includegraphics[width=.34\columnwidth]{projects/2.3.3-MathLibs/2.3.3.13-CLOVER/ginkgo_eas/bw_gmres_solver.pdf}
  \end{tabular}
  \caption{Achieved Bandwidth (GB/s) of the Ginkgo CG (left) and GMRES (right) solvers on one
    GCD of the Crusher MI250X GPUs.}
  \label{fig:gko-crusher-solvers}
\end{figure}
