\subsubsection{\stid{6.01} LANL ATDM Software Technologies}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Overview} \leavevmode \\

The LANL ATDM PMR effort is focusing on the development and use of
advanced programming models for Advanced Technology Development and
Mitigation (ATDM) use-cases. Our current focus is on research and development
of new programming model capabilities in the \textbf{Legion} data-centric
programming system. Legion provides unique capabilities that align
well with our focus on the development of tools and technologies that
enables a separation of concerns of computational physicists and
computer scientists. Within the ATDM PMR effort we have focused on the
development of significant new capabilities within the Legion runtime
that are specifically required to support LANL's ATDM
applications. Another key component of our work is the co-design and
integration of advanced programming model research and development
within \textbf{FleCSI}, a Flexible Computational Science Infrastructure. A
major benefit to the broader ECP community is the development of new 
features in the Legion programming system which are available as free
open-source software \url{https://gitlab.com/StanfordLegion/legion}.  

The \textbf{Kitsune} Project, provides a compiler-focused infrastructure
for improving various aspects of the exascale programming environment.
At present, efforts are primarily focused on advanced LLVM compiler and
tool infrastructure to support the use of a \emph{parallel-aware} intermediate
representation.  In addition, we are actively involved in the Flang Fortran
front-end that is now an official sub-project within the overall LLVM
infrastructure. All these efforts include interactions across ECP as well as
with the broader LLVM community and industry.  


The LANL ATDM \textbf{Cinema} project develops scalable solutions for data analysis as part of the Data and Visualization software stack.
Cinema is a novel database approach to saving data extracts in situ which are then available for post hoc interactive exploration.  These data extracts can include metadata, parameters, data visualizations, small meshes, output plots, etc.  Cinema workflows enable flexible data analysis using a fraction of the file storage.  Cinema ECP workflows that integrate applications, in situ and post-processing analysis are captured and curated through the Pantheon project, which is focused on reproducible ECP workflows.  By integrating E4S caches of both applications and dependent capabilities (Ascent, etc.), Pantheon workflows can be downloaded, built and run quickly enough to be useful in a variety of applications such as CI, prototyping functionality or testing analyses.  

The \textbf{BEE/Charliecloud} subproject is creating software tools to increase portability
and reproducibility of scientific applications on high performance and cloud
computing platforms.  Charliecloud \cite{priedhorskyrrandlestc2016} is an unprivileged Linux container
runtime.  It allows developers to use the industry-standard Docker
\cite{dockerinc}
toolchain to containerize scientific applications and then execute them on
unmodified DOE facility computing resources without paying any performance
penalty.  BEE \cite{beeproject} (Build and Execution Environment) is a toolkit providing
users with the ability to execute application workflows across a diverse set of
hardware and runtime environments.  Using Bee's tools, users can build and
launch applications on HPC clusters and public and private clouds, in
containers or in containers inside of virtual machines, using a variety of
container runtimes such as Charliecloud and Docker. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Key  Challenges} \leavevmode \\

\subparagraph{Legion:}

Applications will face significant challenges in realizing sustained performance on next-generation systems. Increasing system complexity coupled with increasing scale will require significant changes to our current programming model approaches. This is of particular importance for large-scale multi-physics applications where the application itself is often highly dynamic and can exhibit high variability in resource utilization and system bottlenecks depending on what physics are currently in use (or emphasized). Our goal in the LANL ATDM PMR project is to support these highly dynamic applications on Exascale systems, providing improvements in productivity, long-term maintainability, and performance portability of our next-generation applications. 

\subparagraph{FleCSI Legion integration:}
FleCSI is a Flexible Computational Science Infrastructure whose goal is to provide a common framework for application development for LANL's next-generation codes. FleCSI is required to support a variety of different distributed data structures and computation on these data structures including structured and unstructured mesh as well as mesh-free methods. Our work in the LANL ATDM PMR project is focused on co-designing the FleCSI data and execution model with the Legion programming model to ensure the latest advancements in the programming model and runtimes research community are represented in our computational infrastructure. A significant challenge in our work is the additional constraint that FleCSI must also support other runtime systems such as MPI. Given this constraint, we have chosen an approach that ensures functional correctness across both runtimes but that also leverages and benefits from capabilities in Legion that are not directly supported in MPI (such as task-based parallelism as a first-class construct). 

\subparagraph{Kitsune:}
A key challenge to our efforts is reaching agreement within the
broader community that a parallel intermediate representation is
beneficial and needed within LLVM.  This not only requires showing the
benefits but also providing a full implementation for evaluation and
feedback from the community.  In addition, significant new compiler
capabilities represent a considerable effort to implement and involve
many complexities and technical challenges.  These efforts and the
process of up-streaming potential design and implementation changes do
involve some amount of time and associated risk.

Additional challenges come from a range of complex issues surrounding
target architectures for exascale systems.  Our use of the LLVM
infrastructure helps reduce many challenges here since many processor
vendors and system providers now leverage and use LLVM for their
commercial compilers.

\subparagraph{Cinema}
Interfacing to a large number of ECP applications with the Cinema
software and the management of the voluminous data from these
applications. 

\subparagraph{Bee/CharlieCloud}
Other HPC-focused container runtimes exist, such as NERSC's Shifter
\cite{canonrsjacobsend} and
Singularity \cite{kurtzergmsochatvbauermw}.  These alternative runtimes have characteristics, such as
complex setup requirements and privileged user actions, that are undesirable in
many environments.  Nevertheless, they represent a sizable fraction of the
existing HPC container runtime mindshare.  A key challenge for BEE is
maintaining support for multiple runtimes and the various options that they require
for execution.  This is especially true in the case of Singularity, which
evolves rapidly.  Similarly, there is a diverse collection of resources that
BEE and Charliecloud must support to serve the ECP audience.  From multiple HPC
hardware architectures and HPC accelerators such as GPUs and FPGAs, to
differing HPC runtime environments and resource managers, to a multitude of
public and private cloud providers, there is a large set of available resources
that BEE and Charliecloud must take into consideration to provide a
comprehensive solution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Solution Strategy} \leavevmode \\

\subparagraph{Legion:}

In funded collaboration with NVIDIA, LANL and NVIDIA are developing new features in Legion to support our applications. Necessary features are identified through direct engagement with application developers and through rapid development, evaluation, and refactoring within the team. Major features include Dynamic Control Replication for improved scalability and productivity and Dynamic Tracing to reduce runtime overheads for  applications with semi-regular data dependencies such as applications with stencil-based communication patterns. 


\subparagraph{FleCSI Legion integration:}
LANL staff work on co-design and integration of the Legion programming system into the FleCSI framework. We have regular milestones that align well with application needs and the development of new features within Legion. 


\begin{figure}[htb]
  \centering
  \includegraphics[width=4in]{projects/2.3.6-NNSA/2.3.6.01-LANL-ATDM/control-replication-performance}
        \caption{\label{fig:control-replication-performance}\textbf{Productivity features such as Dynamic Control Replication scales well across multi-GPU systems in unstructured mesh computations.}}
\end{figure}

\begin{figure}[htb]
        \centering
        \includegraphics[width=4in]{projects/2.3.6-NNSA/2.3.6.01-LANL-ATDM/tracing-performance}
        \caption{\label{fig:tracing-performance}\textbf{New Legion features such as Tracing will improve strong scaling in unstructured mesh computations.}}
\end{figure}


\subparagraph{Kitsune:}
Given the project challenges, our approach takes aspects of
today's node-level programming systems (e.g. Kokkos) and
programming languages (e.g. C++) into consideration and aims to improve
and expand upon their capabilities to address the needs of ECP and LANL's
mission critical applications.  This allows us to attempt to strike a
balance between incremental improvements to existing infrastructure
and more aggressive techniques that seek to provide innovative
solutions, thereby managing risk while also providing the ability to
introduce new technologies via the toolchain. 

Unlike current designs, our approach introduces the notion of explicit
parallel constructs into the LLVM intermediate representation, building
off of work done at MIT on Tapir~\cite{2.3.6.01:kitsune:Schardl:2017}
and the OpenCILK effort (url{https://cilk.mit.edu}). We are working with
MIT to extend this work as well as making some changes to fundamental data
structures within the LLVM infrastructure to assist with and improve analysis
and optimization passes. 

\subparagraph{Cinema:}
The LANL Cinema project is focused on delivering new
visualization capabilities for creating, analyzing, and managing data for
Exascale scientific applications and Exascale data centers.

Cinema~\cite{cinema:Ahrens:SC14} is being developed in coordination with
LANL's ECP application NGC to ensure that data collected during the simulation
execution is of appropriate frequency, resolution, and viewport for later
analysis and visualization by scientists. Cinema is an innovative way of
capturing, storing and exploring extreme scale scientific data. Cinema is
essential for ECP because it embodies approaches to maximize insight from
extreme-scale simulation results while minimizing data footprint 

\subparagraph{Bee/CharlieCloud:}
The BEE/Charliecloud project is focusing first on providing support for
containerized production LANL scientific applications across all of the
existing LANL production HPC systems.  The BEE/Charliecloud components required
for production use at LANL will be documented, released and fully supported.
Follow-on development will focus on expanding support to additional DOE
platforms.  This will mean supporting multiple hardware architectures,
operating systems, resource managers, and storage subsystems.  Support for
alternative container runtimes, such as Docker, Shifter, and Singularity is
planned.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Recent Progress} \leavevmode \\

\subparagraph{Legion:} 
One of the strengths of Legion is that it executes asynchronous tasks as if they were executed in the sequence they occur in the program. This provides the programmer with a mental model of the computation that is easy to reason about. However, the top-level task in this tree-of-tasks model can often become a sequential bottleneck, as it is responsible for the initial distribution of many subtasks across large machines. In earlier work NVIDIA developed the initial implementation of control replication, which allows the programmer to write tasks with sequential semantics that can be  transparently replicated many times, as directed by the Legion mapper interface, and run in a scalable manner across many nodes.
Dynamic control replication is an important capability for LANL's ATDM effort, allowing our application teams to write applications with apparently sequential semantics while enabling scalability to Exascale architectures. This approach will improve understandability of application code, productivity, and composability of software and ease the burden of optimization and porting to new architectures. 
New dynamic tracing ability has been added to Legion to allow debugging and insight in to peformance optimization activites.

\subparagraph{FleCSI Legion Integration:} 
A key component of LANL's Advanced Technology  Development and Mitigation effort is the development of a flexible computational science infrastructure (FleCSI) to support a breadth of application use cases for our Next Generation Code. FleCSI has been co-designed with the Legion programing system in order to enable our Next Generation Code to be performance portable and scalable to future Exascale systems. Legion provides the underlying distributed and node-level runtime environment required for FleCSI to leverage task and data parallelism, data dependent execution, and runtime analysis of task dependencies to expose parallelism. We completed testing of Legion on Sierra with a Visco-Plastic Self-Consistent, VSCP, application to investigate initial performance on GPU systesm.

\subparagraph{Kitsune:}
Our primary focus is the delivery of capabilities for LANL's ATDM
Ristra application (AD 2.2.5.01).  In support of the requirements for
Ristra, we are targeting the lowering of \textit{``forall''} constructs,
including Kokkos \texttt{parallel\_for} construct, directly into the
parallel representation.  At present, this works for many C++ constructs
(e.g., for and for-range statements).  We can target this code to different
runtimes and architectures via the compiler and thus avoid reimplementaiton
of Kokkos or fundametnall C++ constructs. In addition we are working to replace
LLVM's dominator tree, a key data structure for optimizations including
parallelization and memory usage analysis, with a \emph{dominator directed-acyclic-graph}
(DAG).  This capability is still in its early evaluation state and we continue to
explore correctness and compatiblity within the overall LLVM infrastructure. We are
actively watching recent events within the LLVM community around
multi-level intermediate representations (MLIR) and the relationship
they have with parallel semantics, analysis, optimization, and code
generation.

At present we are successfully compiling our full applications using the new
toolchain.  We continue to test, debug, and work towards improved optimizations
and performance. 

\subparagraph{Cinema:}

Recent Cinema progress has focused on development of exascale workflows, development of python-based Cinema functionality, and supporting Cinema export capabilities through ALPINE's exascale-capable infrastructure.  ParaView's v5.9 release includes a significant rewrite to create \textit{extract generators} to output images and other extracts.  The creation of Cinema databases is part of the extract generator workflow both in post hoc ParaView usage or via the in situ Catalyst library.  Cinema export is also available via ALPINE's Ascent infrastructure and through VisIt.    
Cinema capabilities provide scientists more options in analyzing and exploring the results of large simulations by providing a workflow that 1) detects features in situ, 2) captures data artifacts in Cinema databases, 3) promotes post-hoc analysis of the data, and 4) provides data viewers that allow interactive, structured exploration of the resulting artifacts. 
%
In our milestones during FY20, we extended two end-to-end reproducible simulation pipelines with ECP applications at scale to generate Cinema databases and ran Cinema-based workflows with Cinema algorithms to produce secondary set of artifacts.  We ran (1) Nyx integrated with Ascent, running an ALPINE adaptive sampling algorithm; and (2) SW4 integrated with Ascent, running a VTK-m isocontour algorithm.   We ran scaling and performance testing for typical ECP-based Cinema use cases.  Lastly we did the annual release of the Cinema toolkit.  Based on user feedback, we  changed the toolkit from a set of viewers and a command line tool to a single Python module that includes current Cinema toolkit components such as viewers, database classes, preliminary Composable Image Sets classes, Jupyter notebook classes, and a small web server to provide a new way to view databases with the existing viewers.  The Cinema Python module is also included in ParaView v5.9. An example of a Jupyter notebook-based approach is shown in Figure~\ref{fig:cinemasci-nyxexample} with an ExaSky:Nyx volume displayed within a notebook workflow.   The Cinema team is working with Exascale science applications to develop in situ and post hoc workflows based on data extracts such as in Figure~\ref{fig:alpine-statistical-feature} where bubbles detected in situ are then analyzed within a Cinema viewer to enable studies of bubble dynamics.  



\subparagraph{Bee/CharlieCloud}
% previous recent progress
%Recent Charliecloud progress has been focusing on understanding and documenting
%best practices for running large scale MPI jobs using containerized runtimes.
%Additional work has been done to enhance support for using containers with
%GPUs.  Charliecloud is available at https://github.com/hpc/charliecloud and was
%recently approved for inclusion in the next release of OpenHPC.
%
%BEE currently has beta-level support for launching Charliecloud containers on
%LANL HPC systems.  Automated BEE scalability testing is nearing production
%readiness.
Recent Charliecloud progress has focused on understanding and documenting best
practices for running large scale MPI jobs using containerized runtimes.
Charliecloud is enhancing support for multiple MPI implementations.
Charliecloud is available at https://github.com/hpc/charliecloud and is
distributed inside of Debian and Gentoo Linux distributions as well as being
part of OpenHPC.  Charliecloud won an 2018 R\&D-100 award.

BEE fully supports launching Charliecloud containers on all LANL HPC systems.
It can also launch containers on AWS and OpenStack clouds such as NSF
Chameleon.  BEE also supports interactive launching of jobs with the SLURM
resource manager. BEE was shown at the end of FY19 to support a complex multiphysics application with setup, in situp visualization and checkpoint-restart on a production system at LANL.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Next Steps} \leavevmode \\

\subparagraph{Legion:} 
Focus on hardening and scalability of Legion's Dynamic Control Replication and development of Dynamic Tracing for application use-cases. 

\subparagraph{FleCSI Legion Integration:} 
Support the Ristra Application milestone to run on Sierra and Trinity.

\subparagraph{Kitsune:}
The key next steps for our efforts are to expand our test cases by
increasing the complexity of the codes we're compiling, supporting
additional forms of Kokkos constructs, and support other parallel
constructs that meet the needs of Ristra.  Where possible, we will
explore a broader set of use cases within the ECP community.  This
will be done while also striving to maintain a feature set in Kitsune
that matches the most recent releases of the LLVM infrastructure. We
will continue a quarterly release cycle of the software and also when
feature sets align with our milestones.  This work will go
hand-in-hand with the code generation and optimization for the
exascale system target processors: including CPUs and GPUs on the
target platforms.  The development of associated runtime targets that
can reduce code generation complexity will also be a component of our
future efforts (as needed).

With the addition to Fortran to LLVM via the Flang front end we will
also look to add support for lowering to the parallel IR in those
use cases.

\subparagraph{Cinema:}
In FY21, Cinema will be hardening the Composable Image Sets format to meet user requests and adding functionality to the Python-based components, demonstrating these capabilities with ATDM/ECP data and applications.  Cinema is focusing on outreach to ECP applications to identify new application workflows that can be reasonably made efficient and working on new analysis methods for Cinema users.


\begin{figure}[htb]
	\centering
	\includegraphics[width=4in]{projects/2.3.6-NNSA/2.3.6.01-LANL-ATDM/cinema-jnc-nyx-volume.png}
	\caption{
		A screenshot of an ECP Nyx simulation in the new jupyter notebook-based cinemasci module.  
	\label{fig:cinemasci-nyxexample}
	}
\end{figure}

\subparagraph{Bee/CharlieCloud}
A refactoring of BEE to support an open standard is underway. Support for the Open Workflow standard will allow a base on a well defined workflow description language leveraged by other scientiic communities. This will then be tested on multiple systems to ensure portability.
