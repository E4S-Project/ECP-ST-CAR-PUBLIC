
\subsubsection{\stid{2.08} HPCToolkit} 
\paragraph{Overview} 

The HPCToolkit project is working to develop performance measurement
and analysis tools to help ECP application, library, runtime, and tool
developers understand where and why their software does not fully
exploit hardware resources within and across nodes of extreme-scale
parallel systems. Key deliverables of the project are a suite of
software tools that developers need to measure and analyze the
performance of parallel software as it executes on existing ECP
testbeds and new technologies needed to measure and analyze
performance on forthcoming GPU-accelerated exascale systems.

To provide a foundation for performance measurement and analysis, the
project team is working with community stakeholders, including
standards committees, vendors, and open source developers to improve
hardware and software support for measurement and attribution of
application performance on extreme-scale parallel systems. The
project team has been engaging vendors to improve hardware support
for performance measurement in next-generation GPUs and working
with other software teams to design and integrate new capabilities
into operating systems, runtime systems, communication libraries, and
application frameworks that will enhance the ability of software
tools to accurately measure and attribute code performance on
extreme-scale parallel systems.  Using emerging hardware and software
interfaces for monitoring code performance on both CPUs and GPUs, the project team is
working to extend capabilities to measure and analyze computation, data movement,
communication, and I/O as a program executes to pinpoint scalability
bottlenecks, quantify resource consumption, and assess
inefficiencies.

\paragraph{Key  Challenges}

Today's fastest supercomputers and forthcoming exascale systems all
employ GPU-accelerated compute nodes. Almost all of the 
computational power of GPU-accelerated compute nodes comes from GPUs rather than CPUs.
GPU-accelerated compute nodes have complex memory hierarchies that include 
multiple memory technologies with different bandwidth and latency characteristics. In addition, 
GPU-accelerated compute nodes have non-uniform connections between memories and computational elements (CPUs and GPUs). 
Furthermore, three emerging DOE supercomputers (Perlmutter, Aurora, and Frontier) will feature GPUs from different vendors (NVIDIA, Intel, and AMD). 
There are significant differences in the underlying organization of these GPUs as well as their hardware and software support for performance measurement. For
performance tools, the need to support multiple CPU and GPU architectures significantly increases tool complexity.  At the same time, the
complexity of applications is increasing dramatically as developers
struggle to expose billion-way parallelism, map computation onto
heterogeneous computing elements, and cope with the growing complexity
of memory hierarchies. While application developers can employ
abstractions to hide some of the complexity of emerging parallel
systems, performance tools must be intimately familiar with each of
the features added to these systems to improve performance or
efficiency, develop measurement and analysis techniques that assess
how well these features are being exploited, and then relate these
measurements back to software to create actionable feedback that will
guide developers to improve the performance, efficiency, and
scalability of their applications.

\paragraph{Solution Strategy}

Development of HPCToolkit as part of ECP is focused on preparing it
for production use at exascale by enhancing it in several ways. First,
the team is adding new capabilities to measure and analyze
interactions between software and key hardware subsystems in
extreme-scale platforms, including GPUs and the complex memory hierarchies on GPU-accelerated compute nodes. A major focus of this effort is developing new capabilities for 
measurement and analysis of performance on GPUs.
Second, the team is working to improve performance
attribution given optimized code for a large collection of complex node-level programming
models used by ECP developers, including 
vendor specific programming models such as CUDA, HIP, and Data Parallel C++,
open source community programming models such as OpenMP,
and template-based programming models developed at national laboratories such as LLNL's RAJA and Sandia's Kokkos. To support
this effort, the project team is enhancing the Dyninst binary analysis
toolkit, which is also used by other ECP tools. A major focus of this effort 
is to support analysis of GPU binaries. Third, the team is
improving the scalability of HPCToolkit so that it can be used to
measure and analyze extreme-scale executions. Fourth, the project team
is working to improve the robustness of the tools across the range of
architectures used as ECP platforms. Finally, the project team will
work other ECP teams to ensure that they benefit from HPCToolkit's
capabilities to measure, analyze, attribute, and diagnose performance
issues on ECP testbeds and forthcoming exascale systems.

\newpage
\paragraph{Recent Progress}

\begin{itemize}

\item
The project team developed a unified GPU monitoring substrate. Upon this substrate, they developed support for collecting summary metrics for 
kernel launches, memory copies, and synchronizations on AMD, Intel, and NVIDIA GPUs.


\item
The project team enhanced support for fine-grained measurement of GPU computations using PC sampling on NVIDIA GPUs and binary instrumentation on Intel GPUs. 
HPCToolkit uses instruction-level measurements to reconstruct calling context trees and
parses NVIDIA's extended line map to reconstruct GPU inlining call chains to help developers understand performance of complex GPU kernels.
Figure~\ref{fig:hpctoolkit}(a) shows a calling context for PeleC, an ECP application for adaptive-mesh compressible hydrodynamics code for reacting flows; the highlighted region shows a reconstruction of GPU calling contexts with multiple inlined GPU device functions.

\item
The project team enhanced HPCToolkit for collecting and visualizing traces of both CPU and GPU activity.
Figure~\ref{fig:hpctoolkit}(b) shows a trace of a GPU-accelerated execution of LAMMPS, an ECP application for parallel molecular dynamics simulation, using 512 NVIDIA A100 GPUs. Each GPU kernel is related to the CPU calling context in which it was launched, which helps developers understand the performance of complex applications.

\item
The project team developed a novel approach that utilizes both shared and distributed memory parallelism to analyze and aggregate sparse data representations of performance measurements from every rank, thread and GPU stream in a program execution. 

\item 
The project team improved the reliability of HPCToolkit's hpcrun for monitoring complex dynamic library loading and unloading and interacting with the application when other software tools are present.

\end{itemize}

\begin{figure}[t]
\captionsetup{width=.96\textwidth}
\begin{minipage}[t]{.40\textwidth}
\centering
\includegraphics[width=\textwidth]{projects/2.3.2-Tools/2.3.2.08-HPCToolkit/hpctoolkit-pelec-profiles}
\\(a)
\end{minipage}
\hfill
\begin{minipage}[t]{.46\textwidth}
\centering
\includegraphics[width=\textwidth]{projects/2.3.2-Tools/2.3.2.08-HPCToolkit/hpctoolkit-lammps-traces}
\\(b)
\end{minipage}
\caption{(a) 
HPCToolkit's {\tt hpcviewer} showing a detailed attribution of GPU performance metrics in a 
profile of PeleC.
(b) HPCToolkit's {\tt hpctraceviewer} showing CPU and GPU trace lines for LAMMPS.}
\label{fig:hpctoolkit}
\end{figure}

\paragraph{Preliminary Experiences on Early Access systems}
\begin{description}
\item[HPCToolkit]
The project team is working to deliver tracing and profiling of GPU activity atop AMD's {\tt rocTracer} API and to assess GPU kernel activity using hardware counters using AMD's {\tt rocProfiler} API. 
HPCToolkit builds and runs on the Spock and Crusher early access systems. 
At present, the project team is working closely with AMD to 
resolve issues measuring GPU-accelerated
applications with the {\tt rocTracer} and {\tt rocProfiler} APIs
by providing test cases that highlight issues of concern.

In addition, the project team has been collaborating with AMD to develop
support for monitoring the performance of OpenMP offload in the
runtime for AMD's AOMP compiler. The code is integrated into AMD's development
branch and has been submitted upstream
for integration into LLVM OpenMP.  A development branch of HPCToolkit
uses this support to attribute GPU activity to OpenMP {\tt target}
regions. The HPCToolkit project team is working to integrate this 
emerging capability into a forthcoming release.
\item[Dyninst]
Dyninst builds and runs on the Spock and Crusher early access systems. 
HPCToolkit uses Dyninst to analyze AMD CPU binaries to attribute 
performance measurements to source code contexts, 
including functions, loops, and lines.   
\end{description}

\paragraph{Next Steps}
\begin{itemize}

\item 
Integrate GPU measurement and analysis capabilities using hardware counters.

\item 
Continue to improve solutions for fine-grained measurement of computations on Intel GPUs and explore new solutions for fine-grained measurement on AMD GPUs.

\item 
Continue to refine the reliability and performance of monitoring operations on dynamic libraries.

\item 
Work with the open source community to upstream GPU measurement support
developed by the project team into the community version of the {\tt
libomptarget} offloading library.

\item 
Work with DOE and platform vendors to evaluate and refine software interfaces for measuring GPU performance.

\end{itemize}
