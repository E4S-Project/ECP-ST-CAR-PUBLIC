\subsubsection{\stid{3.15} Sake: Trilinos/PEEKS} \label{subsubsect:trilinos}
\paragraph{Overview} 
Trilinos is a large and widely used toolkit for scientific computing, with many users both at DOE labs, in academia, and in industry. 
This project is focused on making Trilinos ready for exascale. One part is to port a core set of Trilinos packages to relevant architectures 
(including NVIDIA, AMD, and Intel GPUs). The other part is to design algorithms that work well on accelerators and at large scale (the focus of 
the PEEKS sub-project).


\paragraph{Key  Challenges}
Developing scalable iterative (Krylov) solvers for the US leadership supercomputers 
deployed in ECP, we acknowledge two major challenges coming from the hardware 
architecture:
\begin{enumerate}
\item 
Fine-grained parallelism in a single node that has to be exploited efficiently 
by the iterative solver and the preconditioner.
\item
Rising communication and synchronization cost as the
computational power is growing much faster than memory power, resulting in 
increased pressure on the bandwidth of all cache/memory levels.
\end{enumerate}

These challenges require the redesign of existing iterative solvers with respect 
to higher parallelism, a reduced number of 
communication and synchronization points, favoring computations over 
communication, and possibly adopting multiprecision algorithms for efficient hardware 
utilization. 

The Trilinos software itself needs to be adapted to the new architectures. We are sensitive that Trilinos has a large user base and therefore need to minimize any user interface changes. Performance portability across a wide range of pltforms is a challenge.

\paragraph{Solution Strategy}

The primary thrusts of the Trilinos/PEEKS project are:
\begin{enumerate}
  \item Performance portability:
        We plan to rely heavily on the Kokkos and KokkosKernels librariesas they provide kernels that are performance portable across a variety of platforms, including CPU and GPU (NVIDIA, AMD, Intel). There is still much porting work, as some packages rely on UVM (unified memory), which is not widely supported.
  \item Low-synchronization Krylov methods:
    	We will develop and deploy pipelined and 
	communication-avoiding Krylov methods in production-quality code, and 
	we are actively collaborating with the ECP ExaWind project to integrate 
        our new features into their application. Another bottleneck we will address is the orthogonalization needed in GMRES (such as CGS and MGS).
\end{enumerate}

\paragraph{Recent Progress}
\begin{enumerate}
\item Polynomial preconditioning: We implemented and deployed a GMRES-based polynomial preconditioner in Trilinos/Belos. This can be used as a preconditioner by itself, as a smoother in multigrid (MueLu), or be combined (nested) with any existing preconditioner. Our code is portable to both CPU and GPU. We showed the method is robust and it does not need any user knowledge of eigenvalues of the matrix/operator, unlike the Chebyshev method. In collaboration with the Exagraph ECP project, it has been integrated into the Sphynx spectral partitioner (Trilinos/Zoltan2).
\item Low-synch orthogonalization: In collaboration with CU Denver and NREL, we developed and implemented a novel low-synch version of Classical Gram-Schmidt (CGS), called DCGS2. By delaying orthonalization and norms, we reduce the synchronization requirement to once per iteration (even with two passes of CGS). The method is more numerically stable than previous attempts at low-synch orthogonalization. Preliminary results on Summit show that the new DCGS2 outperforms the current CGS2 and MGS methods, and achieves up to 66 Gflop/s on 192 GPUs.
\item  TODO Ichi
\end{enumerate}

\paragraph{Next Steps}
Our next efforts are:
\begin{enumerate}
\item Deploy DCGS2 (or related method) in Trilinos/Belos, likely as an option in GMRES (which is used by many ECP applications).
\item Study Block Krylov methods, which may be particularly well suited to GPUs, and also reduce communication.
\item TODO Ichi
\end{enumerate}

