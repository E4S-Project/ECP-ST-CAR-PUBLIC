\subsubsection{\stid{4.10} DataLib} 

\paragraph{Overview} 

The Data Libraries and Services Enabling Exascale Science (DataLib)
project has been pushing on three distinct and critical
aspects of successful storage and I/O technologies for ECP applications:
enhancing and enabling traditional I/O libraries used by DOE/ECP
codes on leadership platforms, establishing a nascent paradigm of data
services specialized for ECP codes, and working closely with facilities
to ensure the successful deployment of our tools. In FY20-23 we plan to
continue to focus on these three complementary aspects of storage and
I/O technologies, adjusting in response to changing needs and bringing
these three aspects together to provide the most capable products for
end users. DataLib activities ensure that facilities have key production
tools, including tools to debug I/O problems in ECP codes; enable multiple
I/O middleware packages through Mochi and ROMIO; and will provide high
performance implementations of major I/O APIs in use by ECP codes.

We strongly support ECP management’s shift of focus towards
Hierarchical Data Format (HDF). In response to ECP guidance
to prioritize the HDF5 API, we propose to emphasize enhanced HDF5
capabilities for ECP codes on current and future DOE leadership
platforms, strengthening HDF as a core technology for the future. We
propose to shift our focus away from ROMIO and PnetCDF development work
to enable rapid progress on this topic. We will continue to support the
use of Mochi tools for development of data services and I/O middleware,
including assisting other ECP AD, ECP ST, and vendor teams in providing
the best storage services possible for ECP applications. We will also
continue to work closely with the facilities to ensure the availability
and quality of our tools on critical platforms.

The Darshan I/O characterization toolset is an instrumentation tool
deployed at facilities to capture information on I/O behavior of
applications running at scale on production systems. It has become
popular at many DOE facilities and is usually on by default. Darshan
data dramatically accelerates root cause analysis of performance
problems for applications and can also (in some cases) assist in
correctness debugging. Our work in this project focuses on extending
Darshan to new interfaces and ensuring readiness on upcoming
platforms.

The ROMIO and Parallel netCDF (PnetCDF) activities
focus on existing standards-based interfaces in broad use, assisting
in performance debugging on new platforms and augmenting existing
implementations to support new storage models (e.g., burst
buffers). In addition to being used directly by applications, ROMIO
and PnetCDF are also indirectly used in HDF5 and netCDF-4. Our work is
ensuring that these libraries are ready for upcoming platforms and
effective for their users (and ready as alternatives if other
libraries fall short).

The Mochi and Mercury software tools are building blocks for
user-level distributed HPC services. They address issues of
performance, programmability, and portability in this key facet of
data service development. Mercury is being used by Intel in the
development of their DAOS storage service and in other data service
activities, while within ECP the HXHIM and UnifyCR projects also have
plans to leverage these tools. In addition to working with these
stakeholders and ensuring performance and correctness on upcoming
platforms, we are also working with ECP application teams to customize
data services for their needs (e.g., memoization, ML model management
during learning). These are supporting tools that are not represented as 
official products in the ECP ST portfolio.

\paragraph{Key Challenges}

Each of these subprojects has its own set of challenges. Libraries
such as HDF, ROMIO, and PnetCDF have numerous users from over a decade of
production use, yet significant changes are needed to address the
scale, heterogeneity, and latency requirements of upcoming
applications. New algorithms and methods of storing data are required.
%
For Darshan, the challenge is to operate in a transparent manner in
the face of continuing change in build environments, to grow in
functionality to cover new interfaces while remaining lean from a
resource utilization perspective, and to interoperate with other tools
that use similar methods to hook into applications.
%
Mochi and Mercury are new tools, so the challenge in the context of
these tools is to find users, adapt and improve to better support
those users, and gain a foothold in the science community.

\paragraph{Solution Strategy} We have a solution strategy for HDF enhancement, supporting ECP applications and facilities, supporting data services, and integration and software QA.

\subparagraph{HDF Enhancement} HDF is the most popular high-level API for
interacting with storage in the DOE complex, but users express concerns
with the current The HDF Group (THG) implementation. We propose to perform
an independent assessment and systematic software development activity
targeting the highest possible performance for users of the HDF5 API on
ECP platforms of interest.

\subparagraph{Directly Supporting ECP Applications and Facilities} We currently
have ongoing interactions with E3SM (PnetCDF), CANDLE (FlameStore/Mochi),
and ATDM/Ristra (Quantaii/Mochi), and we routinely work with the
facilities as relates to Darshan deployments. Our work with these
teams is targeted on specific use cases that are inhibiting their use
of current pre-exascale systems, such as E3SM output at scale using the
netCDF-4/PIO/PnetCDF preferred code path. We will continue to work with
these teams to address concerns, to maintain portability and performance,
and may develop new capabilities if needs arise. 

\subparagraph{Supporting Data Services} Mochi framework components are in use in
multiple ECP related activities, including in the UnifyCR and Proactive
Data Containers (PDC) in ExaHDF5 (WBS 2.4.x) and in the Distributed
Asynchronous Object Storage and other services in the Intel storage
software stack. The VeloC and DataSpaces teams (WBS 2.4.x and x.y.z as
part of CODAR, respectively) are also strongly considering adoption
of our tools. Mochi components enhance the performance, portability,
and robustness of these packages, and our common reliance on Mochi
components means that as Mochi improves, so do all these users.

\subparagraph{Integration and Software QA} DataLib has actively pursued
integration with the ECP ST software stack through the development
and upstreaming of Spack packages and the development and deployment
of automated testing for DataLib technologies, so we are already well
positioned in this aspect of our work. We anticipate this effort to
continue throughout the FY20-23 timeframe, with the addition of pull
requests submitted to THG to upstream HDF5 enhancements and effort applied
to address identified issues in our technologies as appropriate.

\paragraph{Recent Progress} We have made progress toward the HDF5 VOL plug-in and I/O improvements in E3Sm, xRAGE, and CODAR along with reducing Darshan overhead and improving UCX support for Mercury.

\subparagraph{HDF5 VOL Plug-in} Deliver design of HDF5 VOL plug-in and improve I/O
capabilities for ECP application. HDF5 VOL design is complete, and we have an
initial prototype with which we are performing early performance testing. The
design document and developer notes will be attached to the end of this report
for convenience.

As a reminder, our HDF VOL implementation layers on top of the native VOL
implementation provided by HDF. We capture write operations as a log of
changes, and currently we persist both the description of the writes (in our
documentation simply the metadata, stored in the metadata table) and the
contents of the writes (in our documentation the log data, stored in the log
dataset) as HDF datasets. Additionally, another table (the offset table)
stores the offsets of specific datasets in the metadata table, allowing one to
skip over unrelated datasets when looking for specific log entries. We’re
investigating the storage of metadata in memory, for performance reasons.

New optimizations have been made in the HDF VOL implementation, notably compression
of dataset metadata, that allow this implementation to perform faster than our
own PnetCDF implementation (and much faster than default HDF5 or netCDF-4).

\subparagraph{I/O Improvement in E3SM}
We have previously extracted an I/O kernel from the E3SM code, and
we are now working on a branch of this code that can write directly
into HDF5. As background, the netCDF4 API is missing a key capability
to describe I/O to multiple datasets as a single operation, and
this limits the performance of netCDF4 for many scientific codes
regardless of the underlying I/O API being used (e.g., PnetCDF,
HDF). The E3SM team is aware of this deficiency, but to our knowledge
there is no plan to address it at this time.  Meanwhile, the HDF
Group has been looking at multi-dataset writes of this type, although
it is unclear when the capability might be made part of a production
release. Never the less, working around the netCDF4 deficiencies
allows us to isolate HDF performance/API challenges from the
higher-level netCDF4 ones, and allows us to better explore our HDF
VOL implementation.

Recently we have evaluated blob style storage for E3SM, which
is a method of storing data without transforming into the global
ordering desired for analysis. We have found that this method is
competitive with ADIOS results seen elsewhere, reducing the need
for the E3SM team to continue development of new ADIOS options.

\subparagraph{I/O Improvement in xRAGE}
Many applications use HDF5 to write extremely large
single files used for checkpoint restart and/or data analysis. Even on
pre-exascale systems such as Sierra it can be difficult to achieve high
performance (bandwidth) for these workloads. Applications writing single shared
files must carefully align I/O operations to avoid file locking overheads and
balance I/O operations across multiple writers to ensure high performance. To
help diagnose issues and optimize I/O in production Sierra applications the
DataLib team has been working closely with LANL scientists to employ Darshan
analysis capabilities that capture more detailed information about how HDF5,
MPI-IO and POSIX are being used by the application and lower level software
stacks. LANL has begun using Darshan on Sierra for a variety of applications
including the xRAGE application and have worked with the DataLib team to
identify potential bottlenecks in this and other applications.

\subparagraph{I/O Improvement in CODAR}
Early in the year, we initiated discussions with the CODAR team building
Chimbuko, an effort to create a provenance and performance analysis service for
HPC. Through discussion on their use case, we determined that a new Mochi
microservice was needed. The Sonata microservice has been developed to provide
convenient storage and processing of their JSON record data. We continue to work with
the CODAR team to address scalability concerns and enable their Chimbuko service
to hit production goals.

\subparagraph{Reducing Darshan Overheads} Enhance HDF VOL implementation and evaluate
Darshan and Mochi
performance. We will revisit Darshan overheads on flagship platforms to ensure
correctness and low-overhead operation, and we will incorporate new
enhancements into the HDF VOL implementation for further evaluation. We will
also perform a performance evaluation of key Mochi use cases on available test
hardware.

This work uncovered some overheads in how we capture timing information
on platforms such as Theta. Workarounds were developed to keep
Darshan overheads in the noise for all but the most degenerate I/O
use cases (e.g., single-byte writes via stdio).

\subparagraph{UCX Support for Mercury} We delivered UCX plug-in for Mercury
in support of ECP applications and services: a tested UCX plug-in
for Mercury, tuned for performance on ECP relevant platforms (e.g.,
Summit test systems), with appropriate nightly tests. Our colleagues
working on DAOS have begun testing with this implementation with
promising early results.

\paragraph{Next Steps}

Our near-term plan includes the following actions.
% \emph{STDM12-32:} Evaluate HDF prototype and build new Darshan capabilities. We
% will build a DAOS module and continue evaluation of HDF VOL prototype,
% adjusting design in response.

% \emph{STDM12-33:} Hold Mochi boot camp and release HDF VOL prototype. We will
% hold a Mochi training session, likely at the Annual Meeting, and we will
% release the HDF VOL prototype with a Spack package for others to work with.

% \emph{STDM12-34:} Enhance HDF VOL implementation and evaluate Darshan and Mochi
% performance. We will revisit Darshan overheads on flagship platforms to ensure
% correctness and low-overhead operation, and we will incorporate new
% enhancements into the HDF VOL implementation for further evaluation. We will
% also perform a performance evaluation of key Mochi use cases on available test
% hardware.

\subparagraph{STDM12-36} The ROMIO team will implement a new plug-in for
UnifyFS, revisit the ROMIO implementation overall to reduce
synchronization costs, and work with the HDF5 and UnifyFS teams to
benchmark and document the performance of these optimizations and
possible next steps.

\subparagraph{STDM12-37} We will evaluate and improve HDF VOL performance
on latest available platforms, re-architect Darshan analysis tools
using Python to be both more user friendly and more amenable to
learning methods, and enhance Mochi benchmarks and improve performance
for latest platforms based on STDM12-34 analysis.

\subparagraph{STDM12-38}
We will prepare materials and engage in outreach for DataLib software
at ECP Annual or similar event, refine and enhance Darshan
DAOS and HDF modules on latest available platforms, deep dive
on performance of ATDM code (TBD) with HDF VOL, and develop
new PnetCDF benchmark in conjunction with ECP user and analyze performance.


