\subsubsection*{PowerStack}

\paragraph{Overview} 
Power remains a critical constraint for Exascale. As we design supercomputers with higher heterogeneity at larger scales, power becomes an expensive and limited resource. Inefficient management of power leads to added operational costs as well as low scientific throughput. Although hardware advances will contribute a certain amount towards achieving high energy efficiency, several vendors agree that these will not be sufficient in isolation -- creating a need for a sophisticated system software approach. Significant advances in software technologies are thus required to ensure that Exascale systems achieve high performance with effective utilization of available power. Distributing available power to nodes (and components such as GPUs) while adhering to system, job and node constraints involves complex decision making in software. 

The ECP PowerStack sub-area in Argo explores hierarchical interfaces for power management at three specific levels: batch job schedulers, job-level runtime systems, and node-level managers. Each level will provide options for adaptive management depending on requirements of the supercomputing site under consideration. Site-specific requirements such as cluster-level power bounds, user fairness, or job priorities will be translated as inputs to the job scheduler. The job scheduler will choose power-aware scheduling plugins to ensure compliance, with the primary responsibility being management of allocations across multiple users and diverse workloads. Such allocations (physical nodes and job-level power bounds) will serve as inputs to a fine-grained, job-level runtime system to manage specific application ranks, in-turn relying on vendor-agnostic node-level measurement and control mechanisms. The figure below presents an overview of the envisioned PowerStack, which takes a holistic approach to power management.  Additionally, power management support for science workflows (such as MuMMI Cancer workflow, E3SM climate models, etc), in-situ visualization, and workflow management infrastructures (e.g. Kokkos and Caliper) is being developed. Interfaces with ATDM projects such as LLNL's Flux are also being developed. Furthermore, solutions for co-scheduling challenges for extremely heterogeneous architectures are being designed as a part of a university subcontract to University of Arizona. 

This project is essential for ECP because it enables power management of Exascale applications and science workflows on modern heterogeneous architectures, where optimal performance often depends on how resources are scheduled efficiently across power domains (eg GPUs, or co-scheduling). The project is also essential to allow for better throughput and utilization of such heterogeneous clusters, and for allowing applications to operate safely and optimally with power and energy constraints when needed. This project is also essential for building a sophisticated hierarchical software stack proposed by the ECP ATDM (LLNL) and Flux projects, as well as community standardization efforts such as the PowerAPI standard. Additionally, the project fulfills an essential need for ECP by enabling vendor and academic collaborations that provide for accelerated adoption of best practices and better interoperability at scale. By leveraging the software developed in this project, compute centers can safely operate under power and energy constraints while maximizing performance and scientific throughput. 

\begin{figure}[t]
	\centering
	\includegraphics[scale = 0.7]{projects/2.3.1-PMR/2.3.1.19-Argo-PowerSteering/PowerStack_v2.png}
	\caption{Envisioned PowerStack}
	\label{fig:pstack}
\end{figure}


\paragraph{Key Challenges}
Power management in software is challenging due to the dynamic phase behavior of applications, processor manufacturing variability, and the increasing heterogeneity of node-level components. While several scattered research efforts exist, a majority of these efforts are site-specific, require substantial programmer effort, and often result in suboptimal application performance and system throughput. Additionally, these approaches are not production-ready and are not designed to cooperate in an integrated manner. A holistic, generalizable and extensible approach is still missing in the HPC community, and a goal for the ECP Argo PowerSteering project is to provide a solution for this technology gap. 

Another set of challenges come from portability issues. Existing solutions are targeted toward specific microarchitectures (typically Intel) as well as specific programming models (typically MPI-only and traditional benchmarks). Additionally, some of the existing solutions violate the specified power budget before reaching a steady state, resulting in power fluctuations as well as unsafe operation. As part of this project, we strive to provide portability across multiple platforms (IBM, NVIDIA, ARM, AMD, etc), multiple programming models that enable workflows (through Kokkos or Caliper, or specific science workflow studies such as E3SM or MuMMI). Such portability and support of vendor-neutrality is important for safe operation using both hardware-level and application-level information for adaptive configuration selection and critical path analysis.

\paragraph{Solution Strategy}
As discussed earlier, our solution is to develop an end-to-end deployable stack, that combines coarse-grained power-scheduling (Flux, SLURM) with fine-grained job-level runtime system (Intel GEOPM) while ensuring vendor neutrality through node-level interfaces in Variorum. Such a stack can typically operate transparently to user applications. At the scheduler level, we are working on extending SLURM and Flux resource managers to be power-aware. Here, we are looking at both static, coarse-grained power management and variation-aware scheduling in Flux, as well as portability through SLURM SPANK plugins. For the \emph{job-level}, a power management runtime system called GEOPM that will optimize performance of Exascale scientific applications transparently is being developed in collaboration with Intel. At the node-level, vendor-neutral interfaces are being developed as part of Variorum library, to allow for support for Intel, IBM, AMD, ARM, and HPE/Cray platforms. In order to accomplish portability and smooth integration across domains, we are closely collaborating with ECP MuMMI workflow project, the E3SM workflow project, ECP Flux, Kokkos and Caliper, and with the University of Arizona. We are actively engaging ECP users in order to support power management in a non-intrusive and transparent manner. 

\paragraph{Recent Progress}
We achieved three milestones through FY21 in September 2021. In the first milestone, delivered in January 2021, the LLNL power team added the AMD and ARM architectures to Variorum. Variorum is an extensible library for exposing monitor and control capabilities of low-level hardware knobs. Variorum provides vendor-neutral APIs such that the user can query or control hardware knobs without needing to know the underlying vendor’s implementation. These APIs enable HPC application developers to gain a better understanding of performance through various metrics for the devices contained within the node. Additionally, the APIs may enable system software to control hardware knobs to optimize for a particular goal. Details about Variorum can be found at: \url{https://variorum.readthedocs.io/en/latest/}. Additionally, JSON-based interfaces that allow Variorum to interact with runtime systems and schedulers were also developed and hardened in this milestone, allowing support for interaction with ECP system software such as Kokkos, Caliper, and Flux. The team also hardened the codebase further to include continuous integration, additional documentation, concrete examples and tests.

In the second milestone in FY21, delivered in May 2021, we integrated GEOPM and NRM (Argo's Node Resource Manager) with a co-launch mechanism. We introduced a new wrapper script named \texttt{geopmnrmlaunch} along with its support components to launch GEOPM alongside NRM. The script supports the launch of GEOPM in two modes: the process mode and the thread mode. The goal was to compartmentalize power management decisions as follows: (1) manage job-level power bound and extract job-level power efficiency through GEOPM, and (2) manage node-level resource limiting and optimization goals through NRM. This design provides the basis for several advantages over the legacy PowerStack design. For example, hierarchical assignment of power optimization goals is now possible from the job scheduler to the NRM because GEOPM integrates with the job scheduler (SLRUM). Furthermore, since NRM supports several containerization technologies out of the box, GEOPM can indirectly support containerized workflows with a limitation that power-assignment is specified at power domain boundaries. Additionally, compartmentalization of the power optimization goals enables separating job-level goals from node-level goals. This separation enables us to define level-specific goals, such as improving the time spent on the critical path (Instructions per Second or IPS) at the job level or improving power efficiency at the node level (IPS per Watt or IPS/W). 

In the final milestone in FY21, delivered in Sept 2021, PowerStack team at LLNL and University of Arizona worked on three parallel goals of (1) Designing co-scheduling policies for ECP applications and comparing different policies for throughput (average turnaround times), (2) Integrating Variorum with Caliper in order to enable several Caliper workflows to leverage Variorum, and (3) Extending Variorum JSON API to include power domain queries. By leveraging initial codes and data from FY20, we conducted a detailed analysis of co-scheduling applications using a real HPC trace from LLNL’s Cab cluster. This included an analysis of which applications can be co-scheduled effectively to allow for space and time sharing of resources, and how the trade-off space between node utilization, throughput and individual application performance can be modeled. A scheduling simulator was designed and implemented for the same. We used NAS benchmarks as well as E3SM for co-scheduling studies. The Caliper service for Variorum and the JSON API for power domain queries further strengthened the codebase to allow for integration with higher-level system software. 

Additionally, LLNL continues to work with a multi-vendor team towards a CRADA involving Intel, HPE, ARM and IBM -- industry partners that are helping us drive vendor-neutral solutions to power management; and are actively engaged in the community effort for PowerStack homogenization. We are also working in collaboration with PowerAPI team for the same. We established the PowerStack community charter in June 2018, involving collaborators across multiple vendors (Intel, IBM, ARM, HPE, AMD, NVIDIA, Cray), academic institutions (TU Munich, Univ. Tokyo, Univ. Bologna, Univ. Arizona), and national laboratories (Argonne National Lab).The goal for this team is to design a holistic, flexible and extensible concept of a software stack ecosystem for power management. Over the past 3.5 years, this group is looking at engineering and research challenges, along with RFP/procurement designs through active vendor interaction and involvement. A joint BoF with the PowerAPI team, titled ``Community-Driven Efforts for Energy Efficiency in HPC Software Stack,''  is being held at SC21 (\url{https://sc21.supercomputing.org/presentation/?id=bof163&sess=sess404}). 

\paragraph{Preliminary Experiences on Early Access Systems}
The PowerStack team is working closely with AMD for co-designing and enabling low-level power knobs from the user space with Variorum for both CPUs and GPUs as part of the El Capitan statement of work. An AMD CPU port will be included as part of the Variorum February 2022 release, and the AMD GPU port for Variorum is under development (as some of the hardware supported features are still being sketched out). These ports include both telemetry and control, and are currently being tested on internal AMD nodes. This is because specialized kernel drivers and root privileges are required for such testing and evaluation of power management knobs. We are able to build and expose both telemetry and control, as well as run comparison studies, as described in detail in the report for our February 2022 milestone. Once fully integrated, Variorum will support both Frontier and El Capitan architectures, and will also support power-aware scheduling through Flux. 

\paragraph{Next Steps}
We will continue our research and development work as planned toward the FY22 milestones. More specifically, we will continue development for variorum library to allow better support for AMD CPUs, AMD GPUs and other architectures, along with better JSON APIs for system software integration with Kokkos, Caliper and Flux on El Capitan. We will continue to extend Intel GEOPM's new codebase, progress with advanced integration of GEOPM and NRM, continue development of scheduler components such as Flux and SLURM, work on GPU power capping research, and enable user-space access to power management on diverse architectures. We will expand our collaborations for science workflows, such as MuMMI and E2SM, including support for Caliper and Kokkos power management. We will also continue to further explore co-scheduling challenges in power management (University of Arizona) and multi-tenancy issues in power management on heterogenous architectures, and lead the efforts on multi-vendor CRADA. Additionally, we will forge the pathway for converged computing platforms to adopt the power stack by collaborating on HPC+Cloud initiatives. 
