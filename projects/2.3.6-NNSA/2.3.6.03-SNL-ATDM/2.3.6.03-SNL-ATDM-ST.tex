\subsubsection{\stid{6.03} SNL ATDM Software Technologies}

\paragraph{Overview} \leavevmode \\

The SNL ATDM Software Technologies projects are now aggregated to include Kokkos, Kokkos kernels, VTK-m, and Operating Systems and On-Node Runtime efforts. 

The Kokkos programming model and C++ library enable performance portable on-compute-node parallelism for HPC/exascale C++ applications. Kokkos has been publicly available at http://github.com/kokkos/kokkos since May 2015 and is being used and evaluated by projects at DOE laboratories, PSAAP-II centers, other universities, and organizations such as DoD laboratories. Kokkos library implementation consists of a portable application programmer interface (API) and architecture specific back-ends, including OpenMP, Intel Xeon Phi, and CUDA on NVIDIA GPU. These back-ends are developed and optimized as new application-requested capabilities are added to Kokkos, back-end programming mechanisms evolve, and architectures change.

Kokkos Kernels implements on-node shared memory computational kernels for linear algebra and graph operations, using the Kokkos shared-memory parallel programming model. Kokkos Kernels forms the building blocks of a parallel linear algebra library like Tpetra in Trilinos that uses MPI and threads for parallelism, or it can be used stand-alone in ECP applications. Kokkos Kernels supports several Kokkos backends to support architectures like Intel CPUs, KNLs and NVIDIA GPUs. The algorithms and the implementations of the performance-critical kernels in Kokkos Kernels are chosen carefully to match the features of the architectures. This allows ECP applications to utilize high performance kernels and transfers the burden to Kokkos Kernels developers to maintain them in future architectures. Kokkos Kernels also has support for calling vendor provided libraries where there are optimized kernels available.

VTK-m is a toolkit of scientific visualization algorithms for emerging processor architectures. VTK-m supports the fine-grained concurrency for data analysis and visualization algorithms required to drive extreme scale computing by providing abstract models for data and execution that can be applied to a variety of algorithms across many different processor architectures.  The ECP/VTK-m project is building up the VTK-m codebase with the necessary visualization algorithm implementations that run across the varied hardware platforms to be leveraged at the exascale. We will be working with other ECP projects, such as ALPINE, to integrate the new VTK-m code into production software to enable visualization on our HPC systems.  For the ASC/ATDM program, the VTK-m project will concentrate on support of ATDM applications and ASCâ€™s Advanced Technology Systems (ATS) as well as the ASTRA prototype system at Sandia.  General information about VTK-m as well as source code can be found at: http://m.vtk.org.

The OS and On-Node Runtime project focuses on the design, implementation, and evaluation of operating system and runtime system (OS/R) interfaces, mechanisms, and policies supporting the efficient execution of application codes on next-generation platforms. Priorities in this area include the development of lightweight tasking techniques that integrate network communication, interfaces between the runtime and OS for management of critical resources (including multi-level memory, non-volatile memory, and network interfaces), portable interfaces for managing power and energy, and resource isolation strategies at the operating system level that maintain scalability and performance while providing a more full-featured set of system services. The OS/R technologies developed by this project will be evaluated in the context of ATDM application codes running at large-scale on ASC platforms. Through close collaboration with vendors and the broader community, the intention is to drive the technologies developed by this project into vendor-supported system software stacks and gain wide adoption throughout the HPC community.

\paragraph{Key  Challenges} \leavevmode \\

\subparagraph{Kokkos:} The many-core revolution in computing is characterized by:
(1) a steady increase in the number of cores within individual computer chips;
(2) a corresponding decrease in the amount of memory per core that must be shared by the cores of a chip, and,
(3), the diversity of computer chip architectures.
This diversity is highly disruptive because each architecture imposes different complex
and sometimes conflicting requirements on software to perform well on an architecture.
Application software development teams are confronted with the dual challenges of:
(1) inventing new parallel algorithms for many-core chips,
(2) learning the different programming mechanisms of each architecture, and
(2), creating and maintaining separate versions of their software specialized for each architecture.
These tasks may involve considerable overhead for organizations in terms of time and cost.
Adapting application software to changing HPC requirements is already becoming a large expense for
HPC users and can be expected to grow as the diversity of HPC architectures continues to rise.
An alternative, however, is creating software that is performance portable across current and future architectures.

\subparagraph{Kokkos Kernels:} There are several challenges associated with the Kokkos Kernels work. Part of the complexity arises because profiling tools are not yet full mature for advanced architectures and in this context profiling involves the interplay of several factors which require expert judgment to improve performance.  Another challenging aspect is working on milestones that span a variety of projects and code bases. There is a strong dependence on the various application code development teams for our own team's success. In addition, we face a constant tension between the need for production ready tools and components in a realm where the state-of-the-art is still evolving.

\subparagraph{VTK-m:} The scientific visualization research community has been building scalable HPC algorithms for over 15 years, and today there are multiple production tools that provide excellent scalability \cite{ParaView,Catalyst}. That said, there are technology gaps in data analysis and visualization facing ATDM applications as they move to Exascale.  As we approach Exascale, we find that we can rely less on disk storage systems as a holding area for all data between production (by the simulation) and consumption (by the visualization and analysis). To circumvent this limitation, we must integrate our simulation and visualization into the same workflow and provide tools that allows us to run effectively and capture critical information.

\subparagraph{OS \& ONR:} Exascale challenges for system software span the areas of operating systems, networks, and run time systems.  Container technologies are by now ubiquitous in the cloud computing space, but for High Performance Computing their immense potential has been limited by concerns about compatibility with security models and overhead costs.  As vendors bring forward new network hardware for exascale, both vendors and application programmers lack insight into how applications actually use networks in practice, especially regarding the characteristics of the messages sent in production codes.  As programming models like OpenMP at the node level and MPI at the inter-node level evolve, the particular needs of DOE applications must be addressed in both the development of standards and evaluation of provided run time system implementations.



\paragraph{Solution Strategy} \leavevmode \\

\subparagraph{Kokkos: } The Kokkos team developed a parallel programming model with flexible enough semantics that it can be mapped on a diverse set of HPC architectures including current multi-core CPUs and massively parallel GPUs.
The programming model is implemented using C++ template abstractions, which allow a compile time translation to the underlying programming mechanism on each platform, using their respective primary tool chains.
Compared to approaches which rely on source-to-source translators or special compilers, this way leverages the investment of vendors in their preferred programming mechanism without introducing additional, hard to maintain, tools in the compilation chain.

\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{projects/2.3.6-NNSA/2.3.6.03-SNL-ATDM/kokkos-abstractions.jpg}
\caption{Kokkos Execution and Memory Abstractions}
\end{figure}

\subparagraph{Kokkos Kernels:} The Kokkos Kernels team is taking a staged approach to profiling in regards to target architectures and the algorithms involved. We are also coordinating on a regular basis with the other projects that are involved in our work to minimize impediments. In response to the need for production ready tools, we are focusing on a hierarchical approach that involves producing robust, hardened code for core algorithms while simultaneous pursuing research ideas where appropriate. 
 
\subparagraph{VTK-m:} The VTK-m team is addressing its challenges through
development of portable visualization algorithms for VTK-m and leveraging and
expanding the Catalyst~\cite{Catalyst}  \emph{in situ} visualization library to
apply this technology to ATDM applications on ASC platforms.  VTK-m uses the
notion of an abstract device adapter, which allows algorithms written once in
VTK-m to run well on many computing architectures.  The device adapter is
constructed from a small but versatile set of data parallel primitives, which
can be optimized for each platform~\cite{Blelloch1990}.  It has been shown that
this approach not only simplifies parallel implementations, but also allows
them to work well across many platforms~\cite{Lo2012,Larsen2015,Moreland2015}.

\subparagraph{OS \& ONR:} The OS \& ONR team is buying down risk for the use of containers by demonstrating exemplar application containerizations, e.g., for the ATDM SPARC application.  We work with facilities staff to develop and implement strategies to deploy containers on our HPC systems and with dev-ops teams to ease the developer burden for code teams seeking to use containers.  To better understand network resource utilization, we use an MPI simulator that accepts real network traces of application executions as inputs and provides detailed analysis to inform network hardware vendors and application developers alike.  We participate actively in both the OpenMP Language Committee and MPI Forum.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Recent Progress} \leavevmode \\


\subparagraph{Kokkos:}  Kokkos provided a production quality performance portability abstraction to applications and software technology projects under ATDM and ECP which allows them to run on all currently deployed DOE production compute platforms. FY20, the team supported ATDM and ASC IC code Kokkos adoption, debugging and optimization through in house consulting, participation in design discussions, and optimization of Kokkos capabilities.  In coordination with Argonne and Oak Ridge National Laboratories, significant progress was made in the development of new SYCL and HIP backends for Kokkos, targeting respective exascale architectures of Intel and AMD.  The HIP support is fairly mature, with most commonly used capabilities now available (significant work is continuing to address performance issues).  Significant progress was also made on the SYCL backend, but compiler issues have slowed this progress.  To better support asynchrony in Kokkos, a prototype for Kokkos-Graphs was developed which uses CUDA graphs under the hoods. It is currently under code review for integration into the mainline Kokkos release.  The Kokkos team has also continued its engagement as part of the C++ Standards Committee in support of the DOE HPC community, presented a Linear Algebra proposal to LEWG (the Library Evolution Group) and maturing the MDSPAN implementation. 


\subparagraph{Kokkos Kernels:} The Kokkos Kernels project focused much of its recent work on supporting the L1 milestones for the ATDM SPARC and EMPIRE codes, developing new linear algebra kernels and improving current ones on GPU platforms to support these applications. The primary focus was supporting kernels identified by the Trilinos solver need and continuing the optimization of the Trilinos Tpetra-based solver stack for GPUs. As part of this work, a Kokkos Kernels sparse triangular solver was developed, in support of direct solvers and incomplete factorizations on GPU systems.  This implementation performed favorably to vendor implementations (e.g., 2-13x faster than NVIDIA's sparse triangular solver for relevant problems).  The team also developed a sparse ILU(k) for better preconditioning options on GPUs and a cluster-coloring based Gauss-Seidel preconditioner that reduces the launch overhead and reduces the number of iterations (this will assist the ATDM EMPIRE application as a smoother). In addition, the Kokkos Kernels project also supported the ATDM GEMMA team by continuing the co-development of GPU based dense solver ADELUS for distributed memory systems, especially Sierra runs. This ADELUS code achieved 7.7 PetaFLOPs of performance when run on 7600 GPUs of Sierra, significantly outperforming existing state of the art solvers. 
 

\subparagraph{VTK-m:} The VTK-m team made significant improvements to ParaView/Catalyst in support of ATDM applications. These include support for static builds of ParaView/Catalyst, reductions to the memory footprint of Catalyst when used with SPARC, and customized scripting to simplify \emph{in situ} visualization configuration. The team also began development on a performance-evaluation suite that currently consists of a ``driver'' program that allows the team to test VTK-m and Catalyst performance.  The ``driver'' program can read CGNS and Exodus files and drive Catalyst as if a simulation were producing the data. The code was used to evaluate Catalyst with VTK-m enabled filters on the Vortex GPU hardware.  Finally, the team developed and added a Kokkos device adapter to VTK-m.  This advance will simplify porting VTK-m to ECP architectures and reduce VTK-m developer time.

\subparagraph{OS \& ONR:} The OS \& ONR team had a number of recent accomplishments. The LogGOPSim simulation framework that was enhanced to quantify MPI resource usage was used to characterize ATDM workloads and examine the relationship between MPI resource usage and application performance and scalability. A detailed report examining the communication behavior of the Sandia Parallel Aerodynamics and Reentry Code (SPARC) based primarily on its use of MPI resources was completed. The analysis shows that SPARC's communication behavior compares favorably to two well-studied workloads that have been shown to run efficiently at the scale of our leadership-class systems. We also collaborated with ETH Zurich to merge several recent changes that improve the performance and scalability of the simulator framework into a new open source release. We also investigated the use of unprivileged container builds using Podman to enable building of new container images from the login or service/compute nodes of the target HPC resource directly. Specifically, the team installed and validated the use of Podman in an unprivileged setting on the Sandia Stria system. Initial results showed that the full Sandia ATSE software stack and applications can be built using Podman on Stria, but further improvements and testing will be necessary to move into a full production container build capability.  A full report of the updated container workflow model is highlighted in an SC'20 paper entitled ``Chronicles of Astra: Challenges and Lessons from the First Petascale Arm Supercomputer.'' MPI Forum work has progressed in several key improvements for MPI 4.0. Partitioned communication has been officially added and work continues on improvements for GPU communication. We contributed to multiple improvements on collective operations, feedback on dynamic sessions in MPI, and to improvements for 64-bit support for very large messages. We also contributed to reviewing and providing extensive feedback on updates to the underlying semantics of the MPI specification and have contributed some text as well. We continue to actively participate in multiple working groups and co-lead work on partitioned communication and advanced collectives and persistent operations. We are also actively engaged as a key partner with the hybrid programming models group, contributing work on efficient native MPI API support for GPU architectures. The OpenMP Language Committee work focused on preparing for the release of version 5.1 of the specification.  Our contributions include leading the task parallelism subcommittee and editing/revising two chapters of the specification to ensure correctness.


\paragraph{Next Steps} \leavevmode \\


\subparagraph{Kokkos: } The Kokkos project will continue to provide high quality (production) Kokkos support and consultation for ASC applications and libraries. The work on continuing to mature the Kokkos backends will also continue.  In particular, the team will be working (in collaboration with Oak Ridge National Laboratory) to mature and optimize the HIP backend for exascale platforms using AMD GPUs, (in collaboration with Argonne National Laboratory) to develop and optimize SYCL/DPC++ backend for the exascale platforms using Intel GPUs, and to mature and Optimize the OpenMP Target backend as an alternative to the primary tool chains on the exascale platforms. A particular driver of this backend development this year will be to demonstrate a working Kokkos-based Trilinos solver stack.  The Kokkos team will also continue its engagement with the C++ Standards committee, developing C++17 based API improvements, which will allow the Kokkos Programming Model to be more consistent with C++ and thus reduce the mental load for users.   An example of this is using RangePolicy with team handles as nested loop constructs instead of TeamThreadRange. Furthermore, the team will engage the C++ standards committee to further the adoption of successful Kokkos concepts into the standard, and provide feedback on proposed concurrency mechanisms such as the executors proposal. The team will also continue development of the proposed linear algebra capabilities with actual parallel backends, in order to allow an adoption of linear algebra into the standard by 2023.

\subparagraph{Kokkos Kernels:} The Kokkos Kernels project will continue to develop key optimized kernels for the GPU-based exascale systems and provide high quality support/consultation for ATDM/ASC applications and libraries.  The team will develop and deliver a portable MIS-2 algorithm to support better coarsening schemes in multigrid methods on GPU systems.  The team plans to explore new algorithms for sparse matrix-matrix multiplication that have emerged in the field and develop portable implementations of this algorithms, especially targeting SIERRA and El Capitan platforms (progress on the El Capitan implementation will depend on the maturity of the HIP toolchain).  The team will start developing HIP backend support (targeting El Capitan) for key sparse and dense linear algebra kernels.  The team will continue to actively engage the vendor community (NVIDIA, ARM, AMD) to develop and deliver kernels using Kokkos Kernels as reference implementation in order to better support ASC/ATDM codes and the broader CSE community.
 
\subparagraph{VTK-m:} The ATDM/VTK-m project has transitioned away from building functionality into the VTK-m toolkit to addressing the needs of other ST projects and ATDM applications. The FY21 work will continue to focus on the three key goals of ECP: performance, integration, and quality.  In support of these goals, the team will continue development of a performance-evaluation suite capable of evaluating VTK-m and Catalyst on ATS-2 (or similar) machines; make improvements to the Catalyst Python IDE, enabling the management of ParaView scripts inside Python editing tools such as Jupyter; and continue work to unify and evolve the Phactori scripting language for \emph{in situ} visualization, simplifying configuration of Catalyst for HPC applications.  

\subparagraph{OS \& ONR:} We plan to characterize OS noise behavior in the context of containers during execution of ASC workloads and its impact on performance and scalability. There is also interest in understanding the variability of GPU kernel launch latency. We will continue to work with vendors, facilities, and application/library developers to further leverage container technologies for ASC workloads. We will also continue to contribute to the OpenMP and MPI standards bodies to shape the direction of the OpenMP and MPI parallel programming models to provided needed capabilities for ASC workloads.
