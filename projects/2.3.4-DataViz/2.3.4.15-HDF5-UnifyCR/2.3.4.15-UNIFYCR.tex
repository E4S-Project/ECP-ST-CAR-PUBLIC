\subsubsection{\stid{4.15} UnifyCR -- A file system for burst buffers} 

\paragraph{Overview} 

The view of storage systems for HPC is changing rapidly. Traditional, 
single-target parallel file systems have reached their cost-effective 
scaling limit. As a result, hierarchical storage systems are being designed 
and installed for our nation's next-generation leadership class systems. 
Current designs employ “burst buffers” as a fast cache between compute 
nodes and the parallel file system for data needed by running jobs and 
workflows. Burst buffers are implemented as compute-node local storage 
(e.g., SSD) or as shared intermediate storage (e.g., SSD on shared burst 
buffer nodes).

Because burst buffers present an additional complexity to effectively
using supercomputers, we developed UnifyFS, a user-level file system, 
highly-specialized for shared file access on HPC systems with distributed 
burst buffers.  UnifyFS addresses a major usability 
factor of current and future systems, because it enables
applications to gain the performance advantages from distributed burst buffers 
while providing ease of use similar to that of a parallel file system.
To use UnifyFS from within an MPI application, one only needs to change 
the paths for files that the application uses from the parallel file system 
to the mount point for UnifyFS, \texttt{/unifyfs}. Then the application
performs I/O as it normally would, using POSIX I/O or a high level I/O library, e.g.,
HDF5 or MPI-IO. The UnifyFS library intercepts all I/O operations and 
manages the file data locally on the compute nodes with high performance.

\begin{figure}[htb]
        \centering
        \includegraphics[width=3.5in]{projects/2.3.4-DataViz/2.3.4.15-HDF5-UnifyCR/usingUnifyFS}
        \caption{\label{fig:usingUnifyFS} \textbf{Using UnifyFS} 
Using UnifyFS from an MPI application is as easy as using the parallel file system. 
Simply change the file path to point to the UnifyFS mount point \texttt{/unifyfs}, and
then perform I/O as normal.} 
\end{figure}


\paragraph{Key  Challenges}

The hierarchical storage of current and future HPC systems includes compute-node
local SSDs as burst buffers. This distributed burst buffer design promises
fast, scalable I/O performance because burst buffer bandwidth and capacity
will automatically scale with the compute resources used by jobs and
workflows. However, a major concern for this distributed design is how to
present the disjoint storage devices as a single storage location to
applications that use shared files. The primary issue is that when concurrent
processes on different compute nodes perform I/O operations, e.g., writes,
to a shared file, the data for the file are scattered across the separate
compute-node local burst buffers instead of being stored in a single
location. Consequently, if a process wants to access bytes from the
shared file that exist in the burst buffer of a different compute node,
that process needs to somehow track or look up the information for locating
and retrieving those bytes. Additionally, there is no common interface across
vendors for accessing remote burst buffers, so code for cross-node file
sharing will not be easily portable across multiple DOE systems with
different burst buffer architectures, further increasing programming
complexity to support shared files.

For the reasons outlined above, it is clear that without software support 
for distributed burst buffers, applications
will have major difficulties utilizing these resources. 

\begin{figure}[htb]
        \centering
        %\includegraphics[width=3.5in]{projects/2.3.4-DataViz/2.3.4.15-HDF5-UnifyFS/UnifyFS-overview}
        \includegraphics[width=3.5in]{projects/2.3.4-DataViz/2.3.4.15-HDF5-UnifyCR/UnifyFS-overview}
        \caption{\label{fig:UnifyFS-overview} \textbf{UnifyFS Overview.} Users can give commands in their batch scripts to launch UnifyFS within
their allocation. UnifyFS works transparently with POSIX I/O, common I/O libraries, and
VeloC. Once file operations are intercepted by UnifyFS, they
are handled with specialized optimizations to ensure high performance.}
\end{figure}

\paragraph{Solution Strategy}

To address this concern, we have developed UnifyFS, a user-level file system,
highly-specialized for shared file access on HPC systems with distributed
burst buffers. In Figure \ref{fig:UnifyFS-overview}, we show a high
level schematic of how UnifyFS works. Users load UnifyFS into
their jobs from their batch scripts. Once UnifyFS is instantiated, user
applications can read and write shared files to the mount point just like
they would the parallel file system. File operations to the UnifyFS
mount point will be intercepted and handled with specialized optimizations
that will deliver high I/O performance. 

Because bulk-synchronous I/O dominates the 
I/O traffic most HPC systems, we target our approach at
those workloads. Examples of bulk-synchronous I/O include checkpoint/restart
and periodic output dumps by applications. Thus, UnifyFS  addresses a major usability
factor of current and future systems. We designed UnifyFS such
that it transparently intercepts I/O calls, so it will integrate
cleanly with other software including I/O and checkpoint/restart libraries.
Additionally, because UnifyFS is tailored for HPC systems and workloads,
it can deliver high performance.



\paragraph{Recent Progress}

Recently, the UnifyFS team has focused on support for I/O libraries and on read performance. 
For I/O libraries, we evaluated and improved support for HDF5 and MPI-IO. We rigorously 
tested UnifyFS with extensive test suites from the two I/O libraries and uncovered and
corrected numerous bugs and issues. We also performed a complete overhaul of our metadata
infrastructure to improve read performance. With our old infrastructure, read performance
was limited by the need to look up data locations in a distributed fashion. Now, most
data lookups can be performed locally, resulting in orders of magnitude improvement
in read performance.
Our source code for UnifyFS is available on 
GitHub at \url{https://github.com/LLNL/UnifyFS}. 
%\begin{figure}[htb]
        %\centering
        %%\includegraphics[width=4in]{projects/2.3.4-DataViz/2.3.4.15-HDF5-UnifyFS/milestone2}
        %\includegraphics[width=4in]{milestone2}
        %\caption{\label{fig:milestone2} \textbf{UnifyFS Design.} The UnifyFS
%instance consists of a dynamic library and a UnifyFS daemon that runs
%on each compute node in the job. The library intercepts I/O calls to
%the UnifyFS mount point from applications, I/O libraries, or VeloC and communicates them to the UnifyFS daemon that handles the I/O operation.}
%\end{figure}
%

\paragraph{Next Steps}

For our next year's effort, we are focused on delivering an implementation
of our UnifyFS API that can be used by high level I/O libraries for improved
performance. We will also turn our attention to portability to pre-exascale 
platforms. Additionally, we will continue to improve our support for 
ECP collaborator software including:
I/O libraries HDF5, MPI-IO, PnetCDF, and ADIOS; improving
VeloC; and targeting integration with ECP applications such as
E3SM, GEOS, and Chombo.



